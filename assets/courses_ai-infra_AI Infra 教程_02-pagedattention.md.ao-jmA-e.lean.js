import{_ as c}from"./chunks/ArticleMetadata.B38I6HZO.js";import{_ as s,C as p,c as h,o,k as r,G as g,P as d,a as m,w as u,b,e as k}from"./chunks/framework.DIkCQIk8.js";import"./chunks/md5.BFEskVOY.js";const A=JSON.parse('{"title":"vLLM 核心技术 PagedAttention 原理详解","description":"","frontmatter":{"title":"vLLM 核心技术 PagedAttention 原理详解","author":"Se7en","date":"2025/05/27 23:00","categories":["AI Infra 教程"],"tags":["AI","LLM","Inference","Training"]},"headers":[],"relativePath":"courses/ai-infra/AI Infra 教程/02-pagedattention.md","filePath":"courses/ai-infra/AI Infra 教程/02-pagedattention.md","lastUpdated":1749391951000}'),L={name:"courses/ai-infra/AI Infra 教程/02-pagedattention.md"};function f(t,e,P,v,V,M){const l=c,i=p("ClientOnly");return o(),h("div",null,[e[0]||(e[0]=r("h1",{id:"vllm-核心技术-pagedattention-原理详解",tabindex:"-1"},[m("vLLM 核心技术 PagedAttention 原理详解 "),r("a",{class:"header-anchor",href:"#vllm-核心技术-pagedattention-原理详解","aria-label":'Permalink to "vLLM 核心技术 PagedAttention 原理详解"'},"​")],-1)),g(i,null,{default:u(()=>{var a,n;return[(((a=t.$frontmatter)==null?void 0:a.aside)??!0)&&(((n=t.$frontmatter)==null?void 0:n.showArticleMetadata)??!0)?(o(),b(l,{key:0,article:t.$frontmatter},null,8,["article"])):k("",!0)]}),_:1}),e[1]||(e[1]=d('<p>本文是 vLLM 系列文章的第 2 篇，介绍 vLLM 核心技术 PagedAttention 的设计理念与实现机制。</p><p>vLLM PagedAttention 论文精读视频可以在这里观看：<a href="https://www.bilibili.com/video/BV1GWjjzfE1b" target="_blank" rel="noreferrer">https://www.bilibili.com/video/BV1GWjjzfE1b</a></p><p>往期文章：</p><ul><li><a href="https://mp.weixin.qq.com/s/rVW6jjLQabHGMMwnbIzB7Q" target="_blank" rel="noreferrer">vLLM 快速部署指南</a></li></ul><h2 id="_1-引言-为什么大模型推理的内存管理如此关键" tabindex="-1">1 引言：为什么大模型推理的内存管理如此关键？ <a class="header-anchor" href="#_1-引言-为什么大模型推理的内存管理如此关键" aria-label="Permalink to &quot;1 引言：为什么大模型推理的内存管理如此关键？&quot;">​</a></h2><p>随着大语言模型（LLM）在聊天机器人、代码补全、智能问答等场景中的广泛应用，越来越多的公司开始将其作为核心服务进行部署。但运行这类模型的成本极高。相比传统的关键词搜索请求，处理一次 LLM 推理的代价可能高出十倍以上，而背后的主要成本之一，正是 GPU 内存的使用效率。</p><p>在大多数主流 LLM 中，推理过程需要缓存每一步生成过程中的 Key 和 Value 向量（即 KV Cache），以便后续生成阶段引用。这部分缓存并不会随模型权重一起常驻 GPU，而是随着用户请求的长度动态增长和释放。在高并发场景下，不合理的 KV Cache 管理方式会导致大量内存碎片和资源浪费，最终限制可并发处理的请求数量，拉低整体吞吐量。</p><p>为了解决这一瓶颈，vLLM 引入了一个全新的注意力机制 —— <strong>PagedAttention</strong>。它借鉴了操作系统中的虚拟内存分页技术，将 KV Cache 分块存储在非连续的内存地址中，配合 block-level 的共享与 copy-on-write 机制，极大提升了内存利用率，从而显著提高了模型的吞吐能力。</p><p>vLLM 团队将 vLLM 的推理吞吐量与 <a href="https://huggingface.co/docs/transformers/main_classes/text_generation" target="_blank" rel="noreferrer">HuggingFace Transformers（HF）</a> 和 <a href="https://github.com/huggingface/text-generation-inference" target="_blank" rel="noreferrer">HuggingFace Text Generation Inference（TGI）</a> 进行了对比。评估在两种硬件设置下进行：在 NVIDIA A10G GPU 上运行 LLaMA-7B 模型，以及在 NVIDIA A100（40GB）GPU 上运行 LLaMA-13B 模型。实验结果表明，与 HF 相比，vLLM 的吞吐量最高可达 24 倍，与 TGI 相比，吞吐量最高可达 3.5 倍。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505272016382.png" alt=""></p><p>本文将结合 PagedAttention 的论文<a href="https://arxiv.org/abs/2309.06180" target="_blank" rel="noreferrer">《Efficient Memory Management for Large Language Model Serving with PagedAttention》</a>，深入解析 PagedAttention 的设计理念与实现细节，并说明它是如何有效缓解内存瓶颈，显著提升大模型推理性能的。</p><h2 id="_2-背景知识-llm-推理中-kv-cache-的角色" tabindex="-1">2 背景知识：LLM 推理中 KV Cache 的角色 <a class="header-anchor" href="#_2-背景知识-llm-推理中-kv-cache-的角色" aria-label="Permalink to &quot;2 背景知识：LLM 推理中 KV Cache 的角色&quot;">​</a></h2><p>在大语言模型（LLM）如 GPT、OPT、LLaMA 的推理过程中，一个关键机制是<strong>自回归生成（autoregressive generation）</strong>。这意味着模型会基于用户提供的 prompt（提示词），逐步生成下一个 token，每一步都依赖之前生成的 token。这种生成方式的效率，极大依赖于 Transformer 架构中的<strong>自注意力机制（self-attention）</strong>。</p><p>在 self-attention 中，模型为每个 token 计算三个向量：<strong>Query（Q）</strong>、<strong>Key（K）</strong> 和 <strong>Value（V）</strong>。每生成一个新 token，模型会将当前的 Query 向量与之前所有 token 的 Key 向量进行点积计算注意力分数，再据此对 Value 向量做加权求和。这种计算需要频繁访问此前的 token 信息。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505270815621.png" alt=""></p><p>为了避免重复计算这些历史 Key 和 Value 向量，推理系统通常会将它们缓存下来，称为 <strong>KV Cache（Key-Value 缓存）</strong>。这不仅节省了大量重复计算，也显著提升了推理效率。</p><p>推理过程可以划分为两个阶段：</p><ul><li><p><strong>prefill 阶段</strong>：模型接收完整的用户输入 prompt，并一次性并行计算所有 token 的 Query、Key 和 Value 向量。这一阶段是高度并行的，能够充分利用 GPU 的算力资源，因此属于 <strong>compute-bound（计算受限）</strong>，瓶颈在于算力而非内存。</p></li><li><p><strong>decode 阶段</strong>：此阶段模型开始逐个 token 地生成输出。每一步仅处理一个新的 token，模型首先计算其对应的 Query、Key 和 Value 向量。当前 token 的 Query 会与历史及当前的 Key 向量进行点积计算，通过 softmax 得到 attention 权重后，再与历史及当前的 Value 向量进行加权求和，得到当前 token 的 attention 输出。随后，当前步骤产生的 Key 和 Value 向量会被追加写入 KV Cache，以供后续使用。由于该过程是串行的、难以并行加速，且频繁读写缓存数据，因此整体表现为 <strong>memory-bound（内存受限）</strong>，瓶颈主要在 KV Cache 的存储与访问效率。</p></li></ul><p>可以看出，prefill 阶段更侧重于并行计算，而 decode 阶段的性能很大程度上取决于 KV Cache 的内存管理能力。随着生成的 token 数量增加，KV Cache 会线性增长，并逐步占据大量 GPU 显存。</p><p>下图展示了一个 13B 参数规模的大语言模型在 NVIDIA A100 40GB 显存 GPU 上推理时的显存使用分布。其中：</p><ul><li><strong>Parameters（26GB, 65%）</strong>：指模型的权重参数，这部分在加载模型后常驻显存，大小固定；</li><li><strong>KV Cache（&gt;30%）</strong>：用于存储每个请求的历史 Key 和 Value 向量，随着生成 token 数量动态增长，是最主要的动态内存开销来源；</li><li><strong>Others</strong>：主要指推理过程中暂时产生的中间计算结果（如 activation），生命周期短，通常在计算完一层后即被释放，占用较少。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505270835072.png" alt=""></p><p>从这张图可以直观看出，KV Cache 是模型推理过程中最主要的“动态”显存负担，它不仅大，而且会随着请求数量和序列长度迅速膨胀。因此，如何高效管理 KV Cache，决定了一个推理系统能否支撑大批量并发请求，是提高吞吐量的关键。</p><h2 id="_3-内存管理的挑战-kv-cache-为什么成了-llm-推理的瓶颈" tabindex="-1">3 内存管理的挑战：KV Cache 为什么成了 LLM 推理的瓶颈？ <a class="header-anchor" href="#_3-内存管理的挑战-kv-cache-为什么成了-llm-推理的瓶颈" aria-label="Permalink to &quot;3 内存管理的挑战：KV Cache 为什么成了 LLM 推理的瓶颈？&quot;">​</a></h2><p>当前主流的 LLM 推理系统在 KV Cache 的内存管理上普遍存在 3 类结构性问题：显存占用增长快、内存碎片严重，以及缓存难以复用。</p><h3 id="_3-1-kv-cache-占用迅速增长-极易耗尽显存" tabindex="-1">3.1 KV Cache 占用迅速增长，极易耗尽显存 <a class="header-anchor" href="#_3-1-kv-cache-占用迅速增长-极易耗尽显存" aria-label="Permalink to &quot;3.1 KV Cache 占用迅速增长，极易耗尽显存&quot;">​</a></h3><p>以 OPT-13B 为例，论文指出其每个 token 的 KV Cache 占用约为 <strong>800KB</strong>，计算方式为：</p><blockquote><p>2（Key 和 Value）× 5120（hidden size）× 40（Transformer 层数）× 2 字节（FP16） = 819,200 字节 ≈ 800KB</p></blockquote><p>如果生成完整的 2048 个 token，单个请求的 KV Cache 占用就可达约 1.6GB。在一块 40GB 显存的 A100 GPU 上，这种增长速度意味着仅同时处理少量请求，就可能达到内存瓶颈，直接限制了批处理规模和系统吞吐量。</p><h3 id="_3-2-预分配导致内存碎片严重" tabindex="-1">3.2 预分配导致内存碎片严重 <a class="header-anchor" href="#_3-2-预分配导致内存碎片严重" aria-label="Permalink to &quot;3.2 预分配导致内存碎片严重&quot;">​</a></h3><p>传统推理系统（如 FasterTransformer 和 Orca）通常采用<strong>预分配</strong>策略：请求开始时，就按最大可能生成的长度（如 2048 token）为每个请求分配一整块连续内存空间。这种方式带来 3 类显著的内存浪费：</p><ul><li><strong>已预留但尚未使用的空间（Reserved）</strong>：虽然未来会使用，但当前暂未使用到。</li><li><strong>内部碎片（Internal Fragmentation）</strong>：实际 token 数小于预留长度，剩余空间浪费。</li><li><strong>外部碎片（External Fragmentation）</strong>：不同请求所需内存大小不一，造成内存块间不连续，产生碎片。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505270830601.png" alt=""></p><p>论文中的实验结果显示，再传统系统中真正用于存放 KV Cache 的有效内存占比最低仅约 20.4%，其余全为浪费。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505272126728.png" alt=""></p><h3 id="_3-3-kv-cache-难以共享-内存复用受限" tabindex="-1">3.3 KV Cache 难以共享，内存复用受限 <a class="header-anchor" href="#_3-3-kv-cache-难以共享-内存复用受限" aria-label="Permalink to &quot;3.3 KV Cache 难以共享，内存复用受限&quot;">​</a></h3><p>在 <strong>Parallel Sampling</strong> 或 <strong>Beam Search</strong> 等复杂推理模式中，一个请求可能包含多个生成分支（如多个候选答案），这些分支共享相同的 prompt，因此理论上可以共享其 KV Cache。但传统系统将每个分支的 KV Cache 存放在独立的连续内存中，物理上无法实现共享，只能进行冗余复制，进而显著增加了内存开销。</p><h2 id="_4-核心创新-pagedattention-是什么" tabindex="-1">4 核心创新：PagedAttention 是什么？ <a class="header-anchor" href="#_4-核心创新-pagedattention-是什么" aria-label="Permalink to &quot;4 核心创新：PagedAttention 是什么？&quot;">​</a></h2><p>为了解决 KV Cache 内存管理中的高占用、严重碎片和复用困难等问题，vLLM 提出了一种全新的注意力机制 —— <strong>PagedAttention</strong>。它的核心思想，借鉴自操作系统中广泛应用的<strong>虚拟内存分页机制（Virtual Memory &amp; Paging）</strong>。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505271030527.png" alt=""></p><p>在操作系统中，虚拟内存将程序的地址空间划分为固定大小的“页”（Page），这些页可以映射到物理内存中任意位置，实现非连续分配，从而有效解决了内存碎片和共享问题。PagedAttention 将这一思路引入 KV Cache 的管理中，并带来了 3 项关键改进：</p><ol><li><p><strong>KV Cache 被切分为固定大小的 block</strong>：PagedAttention 将每个序列的 KV Cache 切分为固定大小的 block（默认是 16 个 token），每个 block 存储若干个 token 的 Key 和 Value 向量。这种设计统一了内存分配粒度，使系统能够以更标准化的方式管理 KV Cache 的分配与回收，从而提升内存复用效率，并有效减少内存碎片。</p></li><li><p><strong>block 可以存放在非连续的物理内存中</strong>：与传统的 Attention 不同，PagedAttention 不再要求这些 KV 向量在内存中连续排列，而是通过逻辑 block 与物理 block 的映射，实现非连续存储。映射关系由 block table 维护，它类似于操作系统中的页表，用于记录每个逻辑 block 对应的物理内存位置，确保模型在推理过程中可以正确读取所需的 KV Cache。</p></li><li><p><strong>支持灵活的分配与释放，以及共享机制</strong>：PagedAttention 支持按需分配和回收 block，并允许多个序列共享 block。PagedAttention 使用了 copy-on-write（CoW）机制，允许多个生成样本共享大部分输入 prompt 的 KV Cache，只有当某个分支需要写入新 token 的 KV 数据时，系统才会将相关 block 复制到新的物理位置，从而在保证数据隔离的同时极大地节省显存资源，提升推理效率与吞吐量。</p></li></ol><h2 id="_5-实现细节-vllm-如何利用-pagedattention-实现高效推理" tabindex="-1">5 实现细节：vLLM 如何利用 PagedAttention 实现高效推理 <a class="header-anchor" href="#_5-实现细节-vllm-如何利用-pagedattention-实现高效推理" aria-label="Permalink to &quot;5 实现细节：vLLM 如何利用 PagedAttention 实现高效推理&quot;">​</a></h2><h3 id="_5-1-推理过程中的内存管理" tabindex="-1">5.1 推理过程中的内存管理 <a class="header-anchor" href="#_5-1-推理过程中的内存管理" aria-label="Permalink to &quot;5.1 推理过程中的内存管理&quot;">​</a></h3><p>vLLM 在 decode 阶段使用 PagedAttention 配合 block-level 的内存管理机制，实现了高效、动态的 KV Cache 管理。</p><p>如下图所示，用户请求的 prompt 为 <code>&quot;Four score and seven years ago our&quot;</code>，共包含 7 个 token：</p><ol><li><p><strong>prefill 阶段</strong>：vLLM 为前 7 个 token 分配两个逻辑块 block 0 和 block 1，分别映射到物理块 7 和 1。block 0 存储前 4 个 token，block 1 存储后 3 个 token 及第一个生成 token <code>&quot;fathers&quot;</code>，填充数为 4。</p></li><li><p><strong>decode 阶段 - 生成第 1 个词</strong>：生成 token <code>&quot;brought&quot;</code>，由于 block 1 尚未填满（最多容纳 4 个 token），因此直接将新 KV Cache 写入该块，填充计数从 3 更新为 4。</p></li><li><p><strong>decode 阶段 - 生成第 2 个词</strong>：生成下一个 token，此时 block 1 已满，系统为逻辑块 block 2 分配新的物理块 block 3，并写入 KV Cache，同时更新映射表。</p></li></ol><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505272140169.png" alt=""></p><p>整个过程中，每个逻辑块仅在前一个块被填满后才会分配新的物理块，从而最大程度地减少内存浪费。不难发现，在计算时我们操作的是逻辑块，也就是说，这些 token 在形式上是连续排列的；与此同时，vLLM 会通过 block table 映射关系，在后台将逻辑块映射到实际的物理块，从而完成数据的读取与计算。通过这种方式，每个请求仿佛都在一个连续且充足的内存空间中运行，尽管这些数据在物理内存中实际上是非连续存储的。</p><h3 id="_5-2-支持多样化的推理策略" tabindex="-1">5.2 支持多样化的推理策略 <a class="header-anchor" href="#_5-2-支持多样化的推理策略" aria-label="Permalink to &quot;5.2 支持多样化的推理策略&quot;">​</a></h3><p>PagedAttention 中采用的基于分页的 KV Cache 管理机制，不仅在常规单序列生成中表现出色，也天然适合多种复杂的解码策略。</p><h4 id="_5-2-1-并行采样-parallel-sampling" tabindex="-1">5.2.1 并行采样（Parallel Sampling） <a class="header-anchor" href="#_5-2-1-并行采样-parallel-sampling" aria-label="Permalink to &quot;5.2.1 并行采样（Parallel Sampling）&quot;">​</a></h4><p>在 Parallel Sampling 中，同一个 prompt 会生成多个候选输出，便于用户从多个备选中选择最佳响应，常用于内容生成或模型对比测试。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505232255681.png" alt=""></p><p>在 vLLM 中，这些采样序列共享相同的 prompt，其对应的 KV Cache 也可以共用同一组物理块。PagedAttention 通过引用计数和 block-level 的 copy-on-write 机制实现共享与隔离的平衡：只有当序列出现不同分支时，才会触发复制操作。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505271114181.png" alt=""></p><h4 id="_5-2-2-束搜索-beam-search" tabindex="-1">5.2.2 束搜索（Beam Search） <a class="header-anchor" href="#_5-2-2-束搜索-beam-search" aria-label="Permalink to &quot;5.2.2 束搜索（Beam Search）&quot;">​</a></h4><p>Beam Search 是机器翻译等任务中常见的解码策略。它会维护多个“beam”路径，每轮扩展最优候选并保留 top-k 序列。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505232256066.png" alt=""></p><p>在 vLLM 中，多个 beam 可以共享公共前缀部分的 KV Cache，不仅包括输入 prompt，还包括生成过程中的公共前缀 token。只要这些 beam 的生成路径尚未分叉，它们就会复用相同的物理块。当路径分叉发生后，vLLM 才通过 copy-on-write 机制对共享块进行拆分，从而保证每个 beam 的独立性。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505271120044.png" alt=""></p><h4 id="_5-2-3-共享前缀-shared-prefix" tabindex="-1">5.2.3 共享前缀（Shared Prefix） <a class="header-anchor" href="#_5-2-3-共享前缀-shared-prefix" aria-label="Permalink to &quot;5.2.3 共享前缀（Shared Prefix）&quot;">​</a></h4><p>在许多提示工程实践中，多个请求会以相同的系统提示或 few-shot 示例开头（例如翻译任务中的多个例句）。这些共享前缀同样可以被 vLLM 缓存并复用。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505271121514.png" alt=""></p><h4 id="_5-2-4-混合解码-mixed-decoding" tabindex="-1">5.2.4 混合解码（Mixed Decoding） <a class="header-anchor" href="#_5-2-4-混合解码-mixed-decoding" aria-label="Permalink to &quot;5.2.4 混合解码（Mixed Decoding）&quot;">​</a></h4><p>前面提到的几种解码方式（如 Parallel Sampling、Beam Search、Shared Prefix 等）在内存的共享和访问方式上存在差异，传统系统很难同时高效地处理这些不同策略的请求。而 vLLM 则通过一个通用的映射层（block table）屏蔽了这些差异，让系统能够在统一框架下处理多样化的请求。</p><h3 id="_5-3-推理任务的调度与抢占" tabindex="-1">5.3 推理任务的调度与抢占 <a class="header-anchor" href="#_5-3-推理任务的调度与抢占" aria-label="Permalink to &quot;5.3 推理任务的调度与抢占&quot;">​</a></h3><p>当请求量超过系统处理能力时，vLLM 需要在有限的显存资源下合理调度推理请求，并对部分请求进行抢占（Preemption）。为此，vLLM 采用了 <strong>FCFS（first-come-first-serve）</strong> 的策略，确保请求被公平处理：优先服务最早到达的请求，优先抢占最近到达的请求，避免资源饥饿。</p><p>由于大模型的输入 prompt 长度差异大，输出长度也无法预估，随着请求和生成的 token 增加，GPU 中用于存储 KV Cache 的物理块可能耗尽。此时，vLLM 面临两个核心问题：</p><ol><li><strong>应该回收哪些块？</strong></li><li><strong>如果后续仍需使用，被回收的块如何恢复？</strong></li></ol><h4 id="_5-3-1-块回收策略-all-or-nothing" tabindex="-1">5.3.1 块回收策略：All-or-Nothing <a class="header-anchor" href="#_5-3-1-块回收策略-all-or-nothing" aria-label="Permalink to &quot;5.3.1 块回收策略：All-or-Nothing&quot;">​</a></h4><p>vLLM 中每个序列的所有 KV Cache 块始终一起被访问，因此采用 “<strong>All-or-Nothing</strong>” 的回收策略：要么完全回收一个请求的全部 KV Cache，要么不回收。此外，像 Beam Search 这类单请求中含多个子序列（beam）的情况，这些序列之间可能共享 KV Cache，必须作为一个 “<strong>sequence group</strong>” 整体抢占或恢复，确保共享关系不被破坏。</p><h4 id="_5-3-2-块恢复策略-swapping-与-recomputation" tabindex="-1">5.3.2 块恢复策略：Swapping 与 Recomputation <a class="header-anchor" href="#_5-3-2-块恢复策略-swapping-与-recomputation" aria-label="Permalink to &quot;5.3.2 块恢复策略：Swapping 与 Recomputation&quot;">​</a></h4><p>对于被抢占请求的 KV Cache，vLLM 提供两种恢复机制：</p><ul><li><p><strong>Swapping（交换）</strong>：将被抢占请求的 KV Cache 块从 GPU 移动到 CPU 内存。当有空闲 GPU 块时再从 CPU 恢复回来继续执行。为了控制资源使用，被交换（swapped）到 CPU 内存中的 KV Cache 块数量，永远不会超过 GPU 中物理块的总数。</p></li><li><p><strong>Recomputation（重计算）</strong>：直接丢弃已生成的 KV Cache，待请求恢复后重新计算。Recomputation 可将已生成的 token 与原始 prompt 拼接为新的输入，一次性完成所有 KV Cache 的预填充，其开销显著低于原始 decode 阶段逐 token 生成的方式，随后可以继续后续的 decode 过程。</p></li></ul><p>根据论文中的实验结果，Swapping 更适用于 block size 较大的场景，而 Recomputation 则在不同 block size 下的表现更为稳定。在中等 block size（16 到 64）范围内，两种方式的端到端性能基本相当。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505271139291.png" alt=""></p><h3 id="_5-4-分布式架构" tabindex="-1">5.4 分布式架构 <a class="header-anchor" href="#_5-4-分布式架构" aria-label="Permalink to &quot;5.4 分布式架构&quot;">​</a></h3><p>随着大语言模型（LLM）参数规模的不断扩大，许多模型的总参数量早已超出单张 GPU 的显存上限。因此，必须将模型参数切分并分布在多张 GPU 上，以实现并行执行，这种策略被称为 <strong>模型并行（Model Parallelism）</strong>。这不仅要求模型本身具备良好的并行性，也对系统提出了更高要求——需具备能够协调跨设备内存访问与管理的能力。</p><p>为应对这一挑战，vLLM 构建了一套面向分布式推理的执行架构，原生支持 <a href="https://github.com/NVIDIA/Megatron-LM" target="_blank" rel="noreferrer">Megatron-LM</a> 风格的张量并行策略，并通过统一调度与基于分页的 KV Cache 管理机制，实现了跨 GPU 的高效协同推理与资源共享。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202505272000111.png" alt=""></p><p>在每一步 decode 过程中，调度器首先为每个 batch 中的请求准备控制信息，其中包括：</p><ul><li>输入 token 的 ID；</li><li>以及该请求的块表（block table，即逻辑块到物理块的映射信息）。</li></ul><p>接下来，调度器会将这些控制信息广播给所有的 GPU worker。</p><p>然后，GPU worker 开始使用这些输入 token ID 执行模型推理。在注意力层中，GPU worker 会根据控制信息中提供的块表，读取对应的 KV Cache。在执行过程中，GPU worker 会通过 all-reduce 通信原语同步中间计算结果，这一过程无需调度器参与协调。最后，GPU worker 会将本轮生成的 token（采样结果）发送回调度器。</p><h3 id="_5-5-gpu-kernel-优化" tabindex="-1">5.5 GPU kernel 优化 <a class="header-anchor" href="#_5-5-gpu-kernel-优化" aria-label="Permalink to &quot;5.5 GPU kernel 优化&quot;">​</a></h3><p>PagedAttention 引入了非连续、按块访问 KV Cache 的内存访问模式，而这些访问模式并不适配现有推理系统中的通用 kernel 实现。为此，vLLM 专门实现了一系列 GPU kernel 优化，以充分发挥硬件性能：</p><ul><li><p><strong>融合 reshape 与块写入（Fused reshape and block write）</strong> 在 Transformer 的每一层，新生成的 KV Cache 需要被分块、reshape 成适合按块读取的内存布局，并根据 block table 的位置写入显存。vLLM 将这些操作融合为一个 kernel 执行，避免了多次 kernel 启动的开销，从而提升执行效率。</p></li><li><p><strong>融合块读取与注意力计算（Fusing block read and attention）</strong> vLLM 在 FasterTransformer 的基础上改造了注意力 kernel，使其能够根据 block table 实时读取 KV Cache 并执行 attention 操作。为了确保内存访问的合并性（coalesced access），vLLM 为每个块分配一个 GPU warp 来读取数据，并支持同一 batch 中不同序列长度的处理，增强了执行灵活性。</p></li><li><p><strong>优化块复制操作（Fused block copy）</strong> copy-on-write 机制触发的块复制可能涉及多个非连续内存块。若使用 <code>cudaMemcpyAsync</code>，会造成频繁的小数据移动，效率低下。vLLM 实现了一个融合的 kernel，可将多个块复制操作批量合并为一次 kernel 启动，显著降低了调度开销与执行延迟。</p></li></ul><h2 id="_6-总结" tabindex="-1">6 总结 <a class="header-anchor" href="#_6-总结" aria-label="Permalink to &quot;6 总结&quot;">​</a></h2><p>本文系统梳理了 vLLM 核心技术 PagedAttention 的设计理念与实现机制。文章从 KV Cache 在推理中的关键作用与内存管理挑战切入，介绍了 vLLM 在请求调度、分布式执行及 GPU kernel 优化等方面的核心改进。PagedAttention 通过分页机制与动态映射，有效提升了显存利用率，使 vLLM 在保持低延迟的同时显著提升了吞吐能力。</p><h2 id="_7-参考资料" tabindex="-1">7 参考资料 <a class="header-anchor" href="#_7-参考资料" aria-label="Permalink to &quot;7 参考资料&quot;">​</a></h2><ul><li>How To Reduce LLM Decoding Time With KV-Caching: <a href="https://newsletter.theaiedge.io/p/how-to-reduce-llm-decoding-time-with" target="_blank" rel="noreferrer">https://newsletter.theaiedge.io/p/how-to-reduce-llm-decoding-time-with</a></li><li>Introduction to vLLM and PagedAttention：<a href="https://blog.runpod.io/introduction-to-vllm-and-how-to-run-vllm-on-runpod-serverless/" target="_blank" rel="noreferrer">https://blog.runpod.io/introduction-to-vllm-and-how-to-run-vllm-on-runpod-serverless/</a></li><li>The First vLLM Meetup：<a href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing" target="_blank" rel="noreferrer">https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing</a></li><li>图解大模型计算加速系列之：vLLM核心技术PagedAttention原理：<a href="https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ</a></li><li>The KV Cache: Memory Usage in Transformers：<a href="https://www.youtube.com/watch?v=80bIUggRJf4&amp;t=319s" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=80bIUggRJf4&amp;t=319s</a></li><li>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention：<a href="https://blog.vllm.ai/2023/06/20/vllm.html" target="_blank" rel="noreferrer">https://blog.vllm.ai/2023/06/20/vllm.html</a></li></ul><h2 id="_8-欢迎关注" tabindex="-1">8 欢迎关注 <a class="header-anchor" href="#_8-欢迎关注" aria-label="Permalink to &quot;8 欢迎关注&quot;">​</a></h2><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202503222156941.png" alt=""></p>',94))])}const C=s(L,[["render",f]]);export{A as __pageData,C as default};
