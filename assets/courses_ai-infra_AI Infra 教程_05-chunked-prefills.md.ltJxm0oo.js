import{_ as c}from"./chunks/ArticleMetadata.Cxlq8Gj7.js";import{_ as d,C as k,c as l,o as i,k as t,G as g,P as s,a,w as Q,b as T,e as u}from"./chunks/framework.DIkCQIk8.js";import"./chunks/md5.BFEskVOY.js";const A=JSON.parse('{"title":"Chunked Prefills 分块预填充详解","description":"","frontmatter":{"title":"Chunked Prefills 分块预填充详解","author":"Se7en","date":"2025/07/13 20:00","categories":["AI Infra 教程"],"tags":["AI","LLM","Inference"]},"headers":[],"relativePath":"courses/ai-infra/AI Infra 教程/05-chunked-prefills.md","filePath":"courses/ai-infra/AI Infra 教程/05-chunked-prefills.md","lastUpdated":1752417774000}'),m={name:"courses/ai-infra/AI Infra 教程/05-chunked-prefills.md"},f={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"17.001ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 7514.6 1000","aria-hidden":"true"},y={class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},H={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"44.528ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 19681.4 1000","aria-hidden":"true"};function F(n,e,V,v,L,x){const p=c,h=k("ClientOnly");return i(),l("div",null,[e[17]||(e[17]=t("h1",{id:"chunked-prefills-分块预填充机制详解",tabindex:"-1"},[a("Chunked-Prefills 分块预填充机制详解 "),t("a",{class:"header-anchor",href:"#chunked-prefills-分块预填充机制详解","aria-label":'Permalink to "Chunked-Prefills 分块预填充机制详解"'},"​")],-1)),g(h,null,{default:Q(()=>{var r,o;return[(((r=n.$frontmatter)==null?void 0:r.aside)??!0)&&(((o=n.$frontmatter)==null?void 0:o.showArticleMetadata)??!0)?(i(),T(p,{key:0,article:n.$frontmatter},null,8,["article"])):u("",!0)]}),_:1}),e[18]||(e[18]=s('<p>Chunked-Prefills 分块预填充机制的讲解视频可以在这里观看：<a href="https://www.bilibili.com/video/BV1f2uczGEqt" target="_blank" rel="noreferrer">https://www.bilibili.com/video/BV1f2uczGEqt</a></p><p>本文是 LLM 推理系列的第 5 篇，介绍 Chunked-Prefills 分块预填充机制的原理。</p><p>往期文章：</p><ul><li><a href="https://mp.weixin.qq.com/s/rVW6jjLQabHGMMwnbIzB7Q" target="_blank" rel="noreferrer">vLLM 快速部署指南</a></li><li><a href="https://mp.weixin.qq.com/s/94-kEyHui0BLO5S-80eAiw" target="_blank" rel="noreferrer">vLLM 核心技术 PagedAttention 原理详解</a></li><li><a href="https://mp.weixin.qq.com/s/_FnXC7hiQtwyzU-ISvU0CA" target="_blank" rel="noreferrer">Prefix Caching 详解：实现 KV Cache 的跨请求高效复用</a></li><li><a href="https://mp.weixin.qq.com/s/sdIt8PpZDZ8DB8iKJ4xoEA" target="_blank" rel="noreferrer">Speculative Decoding 推测解码方案详解</a></li></ul><h2 id="_1-传统-prefill-和-decode-阶段中存在的问题" tabindex="-1">1 传统 prefill 和 decode 阶段中存在的问题 <a class="header-anchor" href="#_1-传统-prefill-和-decode-阶段中存在的问题" aria-label="Permalink to &quot;1 传统 prefill 和 decode 阶段中存在的问题&quot;">​</a></h2><p>每个大语言模型（LLM）的推理请求都会经历两个阶段：首先是 <strong>prefill</strong> 阶段，模型会处理整个输入 prompt 并生成第一个输出 token；随后进入 <strong>decode</strong> 阶段，模型会一次生成一个 token，直到输出完整的结果。</p><p>prefill 阶段由于可以并行处理整个输入，<strong>往往延迟较高但能充分利用 GPU 计算资源</strong>；而 decode 阶段每次只处理一个 token，<strong>延迟较低但 GPU 利用率也随之降低</strong>。**因此，批处理（batching）在 decode 阶段尤为有效，有助于提升整体吞吐量。**但在实际服务中，多个请求的批量调度会导致 prefill 和 decode 阶段交错进行，这为同时实现高吞吐和低延迟带来了不小的挑战。</p><h2 id="_2-batching-的演进过程" tabindex="-1">2 Batching 的演进过程 <a class="header-anchor" href="#_2-batching-的演进过程" aria-label="Permalink to &quot;2 Batching 的演进过程&quot;">​</a></h2><h3 id="_2-1-static-batching" tabindex="-1">2.1 Static Batching <a class="header-anchor" href="#_2-1-static-batching" aria-label="Permalink to &quot;2.1 Static Batching&quot;">​</a></h3><p><strong>Static Batching</strong> 是一种传统的大模型推理调度策略，其核心特点是：一旦构建了一个 batch，<strong>其中的所有请求将统一执行，直到全部完成后才释放资源并加入新的请求</strong>。</p><p><strong>Static Batching 虽然可以降低 TBT 延迟，但也会牺牲整体系统吞吐量，并导致 GPU 资源浪费。</strong></p><p>下图展示了使用 Static Batching 完成 4 个推理请求的过程。在第一轮（图左），每个请求根据提示词（黄色）生成一个 token（蓝色）。经过多轮迭代后（图右），每个请求的生成长度不同，因为它们在不同轮次生成了结束标记（红色）。尽管请求 3 在第二轮就已完成，Static Batching 仍要求整个 batch 等待最慢的请求完成（此例中为第六轮的请求 2），这导致 GPU 在后续迭代中无法被充分利用。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052031934.png" alt=""></p><p>下面这张动图可以更清晰地展示 Static Batching 的基本原理：</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052037008.webp" alt=""></p><p>Static Batching 采用的是固定的调度流程：调度器（Scheduler）每次从请求队列中取出一组请求（例如图中的 x1 和 x2），组成一个新的 batch，并交由执行引擎（Execution Engine）统一进行推理。只有当执行引擎完成该 batch 中所有请求的推理后，调度器才会开始处理下一轮 batch。由于 batch 中的所有请求必须一起行动，我们管这种调度策略叫 <strong>request-level schedule。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052043204.png" alt=""></p><p><a href="https://www.usenix.org/system/files/osdi22-yu.pdf" target="_blank" rel="noreferrer">图片来源：Orca: A Distributed Serving System for Transformer-Based Generative Models</a></p><p>以下是 Static Batching 的伪代码实现：</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052058111.png" alt=""></p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><h3 id="_2-2-continuous-batching" tabindex="-1">2.2 Continuous Batching <a class="header-anchor" href="#_2-2-continuous-batching" aria-label="Permalink to &quot;2.2 Continuous Batching&quot;">​</a></h3><blockquote><p>论文：</p><p>Orca: A Distributed Serving System for Transformer-Based Generative Models：<a href="https://www.usenix.org/conference/osdi22/presentation/yu" target="_blank" rel="noreferrer">https://www.usenix.org/conference/osdi22/presentation/yu</a></p></blockquote><p>业界意识到传统方法存在效率低下的问题，并提出了更优的方案。<a href="https://www.usenix.org/conference/osdi22/presentation/yu" target="_blank" rel="noreferrer">Orca: A Distributed Serving System for Transformer-Based Generative Models</a> 是 OSDI ’22 上发表的一篇论文，这是首个系统性解决该问题的工作。Orca 引入了 <strong>iteration-level scheduling</strong>，不再等待 batch 中所有序列生成完成，而是<strong>每轮迭代动态决定 batch 大小</strong>。这样一来，batch 中的某个序列一旦完成生成，就可以立即被替换为新的请求，从而相比 Static Batching 显著提升了 GPU 的利用率。</p><p>下图展示了使用 Continuous Batching 完成 7 个推理请求的过程。左图展示的是第一轮迭代后的 batch，右图展示的是多轮迭代后的情况。一旦某个请求生成了结束标记（EOS token），就将其替换为一个新的请求（例如 S5、S6 和 S7）。这种方式避免了等待所有请求完成后再处理新请求的情况，因此能显著提升 GPU 的利用率。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052120170.png" alt=""></p><p>下面这张动图可以更清晰地展示 Continuous Batching 的基本原理：</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052136736.webp" alt=""></p><h4 id="_2-2-1-iteration-level-scheduling" tabindex="-1">2.2.1 Iteration-Level Scheduling <a class="header-anchor" href="#_2-2-1-iteration-level-scheduling" aria-label="Permalink to &quot;2.2.1 Iteration-Level Scheduling&quot;">​</a></h4><p>下展示了 ORCA 采用迭代级调度（iteration-level scheduling）时的系统架构与整体工作流程：</p><ul><li>① 调度器首先决定下一步要运行哪些请求。</li><li>② 然后调用引擎对四个选中的请求（x₁, x₂, x₃, x₄）进行推理。对于首次被调度的请求，调度器会提供其输入 token 给引擎处理。本例中，x₃ 和 x₄ 尚未进行过任何迭代，因此调度器为 x₃ 提供了 (x₃₁, x₃₂)，为 x₄ 提供了 (x₄₁, x₄₂, x₄₃)。</li><li>③ 接着，引擎对这四个请求执行了一轮模型推理。</li><li>④ 并返回每个请求生成的一个输出 token（x₁₅, x₂₃, x₃₃, x₄₄）。</li></ul><p>一旦某个请求完成推理，请求池会移除该请求，并通知终端返回响应。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507052136287.png" alt=""></p><p><a href="https://www.usenix.org/system/files/osdi22-yu.pdf" target="_blank" rel="noreferrer">图片来源：Orca: A Distributed Serving System for Transformer-Based Generative Models</a></p><h4 id="_2-2-2-在-iteration-level-scheduling-中实现-batching-的挑战" tabindex="-1">2.2.2 在 Iteration-Level Scheduling 中实现 Batching 的挑战 <a class="header-anchor" href="#_2-2-2-在-iteration-level-scheduling-中实现-batching-的挑战" aria-label="Permalink to &quot;2.2.2 在 Iteration-Level Scheduling 中实现 Batching 的挑战&quot;">​</a></h4><p>在实践中应用 iteration-level scheduling 时，我们面临的一个主要挑战就是如何实现批处理。为了达到高效执行的目的，执行引擎应当能够对任意被选中的一组请求进行批处理执行。否则，就只能一个一个地处理请求，无法发挥 GPU 的大规模并行计算能力。</p><p>然而，即使只是两条请求，也无法保证它们在下一轮迭代中能够合并执行。<strong>这是因为要实现批处理，不仅需要多个请求处于相同的阶段，还必须具有形状完全一致的输入张量。</strong></p><p>一对请求在以下 3 种情况下，下一次迭代不能一起批处理：</p><ul><li><strong>两个请求都处于 prefill 阶段，但输入 token 数量不同（例如 x₃ 和 x₄）</strong>。prefill 阶段的 Attention 是一次性并行处理整个 prompt 序列。如果两个请求的输入 token 长度不同，它们的输入张量在长度维度（L）上不一致，无法拼接成统一形状的 batch 张量 <code>[B, L, H]</code>，导致无法合批执行。请求 x₃ 的 prompt 长度是 2，输入张量形状为 <code>[1, 2, H]</code>；请求 x₄ 的 prompt 长度是 3，输入张量形状为 <code>[1, 3, H]</code>，无法拼接成一个 <code>[2, L, H]</code> 的张量，因此不能合批执行。</li><li><strong>两个请求都处于 decode 阶段，但正在生成不同位置的 token（例如 x₁ 和 x₂）。</strong> 虽然 decode 阶段每次只处理一个 token，输入张量形状都是 <code>[1, H]</code>，但此阶段的 Attention 会依赖之前生成的所有 token（即使用 KV cache）。如果请求的生成位置不同，其 KV cache 长度也不同，导致 Attention 的 Key/Value 张量形状不同。</li><li><strong>两个请求处于不同阶段：一个在 prefill，另一个在 decode（例如 x₁ 和 x₃）。</strong> prefill 的一次迭代会并行处理所有输入 token，以提高效率，而 decode 阶段的一次迭代则只处理一个 token。</li></ul><p><strong>为了解决上述挑战，一个可行的思路是：尽可能寻找这些请求在计算过程中的共性，以便将相同的部分合并执行，从而最大化批处理效率；对于差异部分，则单独处理。</strong></p><h4 id="_2-2-3-selective-batching" tabindex="-1">2.2.3 Selective Batching <a class="header-anchor" href="#_2-2-3-selective-batching" aria-label="Permalink to &quot;2.2.3 Selective Batching&quot;">​</a></h4><p><strong>Selective Batching 的核心原理在于：仅对适合批处理的操作执行批处理，不适合批处理的操作则单独处理。</strong></p><p>具体来说：</p>',43)),t("ul",null,[t("li",null,[e[2]||(e[2]=a("对于 ",-1)),e[3]||(e[3]=t("code",null,"preproj",-1)),e[4]||(e[4]=a("、",-1)),e[5]||(e[5]=t("code",null,"postproj",-1)),e[6]||(e[6]=a("、",-1)),e[7]||(e[7]=t("code",null,"FFN1",-1)),e[8]||(e[8]=a(" 和 ",-1)),e[9]||(e[9]=t("code",null,"FFN2",-1)),e[10]||(e[10]=a(" 这类线性变换或归一化操作，它们的计算与序列长度无关，只是在 hidden_size 维度上做线性转换，并且都需要从显存读取权重。",-1)),e[11]||(e[11]=t("strong",null,"因此，可以将 batch 内所有 token 拉平成一个二维张量",-1)),e[12]||(e[12]=a("，例如 x₃ 和 x₄ 的输入张量可以合并为一个形状为 ",-1)),t("mjx-container",f,[(i(),l("svg",b,e[0]||(e[0]=[s('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(278,0)"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1500.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2181.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2626.3,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3514.3,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4070.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5125.9,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(5403.9,0)"><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5903.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6348.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7236.6,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z" style="stroke-width:3;"></path></g></g></g>',1)]))),e[1]||(e[1]=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mo",{stretchy:"false"},"["),t("mo",null,"∑"),t("mi",null,"L"),t("mo",null,","),t("mi",null,"H"),t("mo",{stretchy:"false"},"]"),t("mo",null,"="),t("mo",{stretchy:"false"},"["),t("mn",null,"5"),t("mo",null,","),t("mi",null,"H"),t("mo",{stretchy:"false"},"]")])],-1))]),e[13]||(e[13]=a(" 的二维张量，一次性完成所有相关计算。这样不仅简化了操作，还能显著提升权重加载的利用率，降低 IO 次数，提高整体执行效率。",-1))]),e[14]||(e[14]=t("li",null,[a("对于 Attention 操作，由于每个请求的 mask、KV cache 和 token 位置可能不同，导致其张量形状不一致，无法直接合并处理。Selective Batching 会在进入 Attention 之前将 batch 拆分，逐个请求单独计算 Attention 分数，完成后再将结果合并回统一的张量，以便继续执行后续操作。"),t("strong",null,"Attention 分数的计算并不依赖显存中的模型权重，只需使用之前生成的 Q、K、V 向量即可，因此拆分处理不会带来额外的 IO 开销。")],-1))]),e[19]||(e[19]=s(`<p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507060933382.png" alt=""></p><p><a href="https://www.usenix.org/system/files/osdi22-yu.pdf" target="_blank" rel="noreferrer">图片来源：Orca: A Distributed Serving System for Transformer-Based Generative Models</a></p><h5 id="_2-2-3-1-为什么在-batch-中混合-prefill-和-decode-请求可以提升性能" tabindex="-1">2.2.3.1 为什么在 Batch 中混合 Prefill 和 Decode 请求可以提升性能？ <a class="header-anchor" href="#_2-2-3-1-为什么在-batch-中混合-prefill-和-decode-请求可以提升性能" aria-label="Permalink to &quot;2.2.3.1 为什么在 Batch 中混合 Prefill 和 Decode 请求可以提升性能？&quot;">​</a></h5><p><strong>在 Batch 中混合 Prefill 和 Decode 请求可以提升性能，原因在于这两种阶段对 GPU 资源的利用方式互补，混合后能更充分地发挥硬件潜力</strong>：</p><ul><li><strong>prefill 阶段是计算密集型（compute-bound）</strong>，主要时间花在大规模的线性变换和矩阵运算上，算力利用率高，但内存带宽利用率不高。即使 batch size 很小，prefill 吞吐量也很快趋于饱和，增大 batch size 对提升吞吐帮助有限（比如 batch size 从 4 增加到 8），甚至可能因算力饱和而下降。</li><li><strong>decode 阶段是内存密集型（memory-bound）</strong>，大部分时间消耗在读取 KV cache 和模型权重上，算力利用率很低。此时增大 batch size 可以显著提升吞吐，因为可以合并多次权重和 KV cache 的读取，减少 IO 次数，让空闲的算力得到利用。</li></ul><p>混合批处理的优势在于：</p><ul><li><strong>prefill 阶段可以搭载（piggyback）在 decode 阶段未被充分利用的算力上，提升整体算力利用率。</strong></li><li>decode 阶段可以和 prefill 阶段共享一次权重读取，减少内存带宽压力，提高带宽利用率。</li><li>这样，GPU 的计算单元和内存带宽都能被更充分利用，整体吞吐和 QPS 明显提升。</li></ul><p>下展示了吞吐量随 batch size 变化的趋势。可以观察到：<strong>对于 decode 阶段，吞吐量几乎呈线性随 batch size 增长</strong>；而 prefill 阶段即便只处理一个请求，其吞吐量也已接近饱和，进一步增大 batch size 效果有限。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061144040.png" alt=""></p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><p>prefill 和 decode 阶段在吞吐量扩展性上的差异，源于它们所执行的矩阵乘法形式不同：**prefill 阶段执行的是（批量的）矩阵-矩阵乘法（matrix-matrix multiplications），而 decode 阶段执行的是向量-矩阵乘法（vector-matrix multiplications）。**众所周知，当算术强度高于 GPU 的 <strong>FLOPS:内存带宽</strong>（FLOPS，floating-point operations per second，表示模型在推理过程中执行的浮点运算次数）比值时，内核属于 compute-bound 类型，能够高效地在 GPU 上运行。相反，算术强度较低的内核则由于受限于内存带宽，难以充分发挥 GPU 的计算能力，属于 memory-bound 类型。</p><p>下图展示了 prefill 和 decode 阶段中各个操作的算术强度（arithmetic intensity）。如下图所示，在 prefill 阶段，<strong>即使 batch size 为 1，所有操作的算术强度依然很高</strong>。而在 decode 阶段，这些操作的算术强度下降了两个数量级以上，<strong>只有在 batch size 达到 256 这种极大值时，decode 阶段才开始变得计算密集</strong>。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061434305.png" alt=""></p><p><strong>然而，将 batch size 扩展到如此之高在实际中几乎无法实现，因为每条请求的 KV cache 占用非常大。</strong> 例如，在 LLaMA-13B 模型上，使用 A6000 GPU，在序列长度为 1K 的情况下，最多只能容纳 18 条请求的 batch。<strong>因此，在当前可行的 batch size 范围内，decode 阶段仍然是内存瓶颈（memory-bound）</strong>。</p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><h5 id="_2-2-3-2-selective-batching-还存在什么问题" tabindex="-1">2.2.3.2 Selective Batching 还存在什么问题？ <a class="header-anchor" href="#_2-2-3-2-selective-batching-还存在什么问题" aria-label="Permalink to &quot;2.2.3.2 Selective Batching 还存在什么问题？&quot;">​</a></h5><p>回顾 ORCA 的 Selective Batching 的策略就会发现，其行为具有一定的<strong>随机性</strong>：一个 batch 中包含多少条 prefill 请求、多少条 decode 请求，并没有明确控制，仅仅是按照“先到先服务”的策略动态拼装而成。这就带来一些问题：</p><ul><li>若某个 batch 中包含大量 prefill 请求，或某些 prefill 请求本身 token 很长，就会导致 prefill tokens 占据大量计算资源，使整个 batch 变得 <strong>compute-bound</strong>；</li><li>相反，若 batch 中以 decode 请求为主，例如所有请求都处于推理阶段，或没有新的输入序列可调度，则该 batch 很可能是 <strong>memory-bound</strong> 的，导致算力无法充分利用。</li><li>在流水线并行中同样可能产生气泡。</li></ul><p>虽然流水线并行（Pipeline Parallelism）可以扩展大模型的并行能力，但也引入了一个典型问题：<strong>流水线气泡（pipeline bubbles）</strong>。所谓“气泡”，是指由于不同阶段间计算不均衡或等待导致的 GPU 空闲时间，从而造成资源浪费和吞吐下降。</p><p>Orca 系统尝试通过 <strong>迭代级调度（iteration-level scheduling）</strong> 来缓解这一问题，但在实际推理中仍然可能出现气泡，主要原因包括：</p><ul><li><strong>PB1：连续 micro-batch 的 prefill token 数量差异大</strong>。例如，若 AB 和 CD 分别是两个 micro-batch，且 AB 的 token 总数显著多于 CD。当 GPU1 完成 Cp 和 Dp 的 prefill 后，必须等待 GPU2 完成 AB 的 prefill，才能继续执行 Ad1 和 Bd1 的 decode。GPU1 在此期间处于空转状态，形成 PB1 类型气泡。</li><li><strong>PB2：prefill 阶段和 decode 阶段计算负载差异大</strong>。PB2 类型气泡出现在 prefill 和 decode 阶段相继执行时。以 Ad1 和 Bd1 为例，它们的 decode 阶段每次仅处理一个 token，计算时间极短；而此时 GPU2 正在处理 Cp 和 Dp 的 prefill，涉及多个 token，耗时较长，导致 GPU1 无法及时执行后续任务，资源被浪费，形成 PB2 气泡。</li><li><strong>PB3：decode 阶段上下文长度差异导致计算时间不均</strong>。decode 阶段的计算开销受上下文长度（即 KV cache 长度）影响较大。不同 micro-batch 中请求的上下文长度不一，导致 decode 阶段耗时不同，从而在流水线上产生等待，形成 PB3 类型气泡。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061505954.png" alt=""></p><p><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">图片来源：SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills</a></p><h3 id="_2-3-chunked-prefills" tabindex="-1">2.3 Chunked-Prefills <a class="header-anchor" href="#_2-3-chunked-prefills" aria-label="Permalink to &quot;2.3 Chunked-Prefills&quot;">​</a></h3><blockquote><p>论文：</p><p>SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills： <a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">https://arxiv.org/abs/2308.16369</a></p><p>Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve：<a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">https://arxiv.org/abs/2403.02310</a></p><p>Github：<a href="https://github.com/microsoft/sarathi-serve" target="_blank" rel="noreferrer">https://github.com/microsoft/sarathi-serve</a></p></blockquote><p>为了进一步解决上述问题，Sarathi-Serve 提出了一种兼顾吞吐量与延迟的调度机制，其中包括两个核心设计思想：<strong>chunked-prefills（分块预填充）</strong> 和 <strong>stall-free scheduling（无阻塞调度）</strong>。chunked-prefills 将一个 prefill 请求拆分为计算量基本相等的多个块（chunk），并在多轮调度迭代中逐步完成整个 prompt 的 prefill 过程（每次处理一部分 token）。而 stall-free scheduling 则允许新请求在不阻塞 decode 的前提下，动态加入正在运行的 batch，通过将所有 decode 请求与新请求的一个或多个 prefill chunk 合并，<strong>构造出满足预设大小（chunk size）的混合批次</strong>。</p><p>Sarathi-Serve 建立在 iteration-level batching 的基础上，但有一个重要区别：它在接纳新请求的同时，限制每轮迭代中 prefill token 的数量。<strong>这样不仅限制了每轮迭代的延迟，还使其几乎不受输入 prompt 总长度的影响。<strong>通过这种方式，Sarathi-Serve 将新 prefill 的计算对正在进行的 decode 阶段的 TBT 影响降到最低，从而</strong>同时实现了高吞吐量和较低的 TBT 延迟</strong>。</p><p><strong>此外，Sarathi-Serve 构建的混合批次（包含 prefill 和 decode token）具有近似均衡的计算需求</strong>。结合流水线并行（pipeline-parallelism），这使我们能够创建基于微批处理（micro-batching）的均衡调度，从而显著减少流水线气泡（pipeline bubbles），提升 GPU 利用率，实现高效且可扩展的部署。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061639315.png" alt=""></p><p><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">图片来源：SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills</a></p><p>在实际调度过程中，<strong>Sarathi-Serve 会优先调度正在进行的 decode 请求</strong>，因为每个 decode 仅消耗一个 token，且对延迟最为敏感，调度器会根据 KV cache 的容量判断是否仍可继续添加 decode 请求。随后，<strong>系统会在剩余的 token 预算范围内处理尚未完成的 prefill 请求</strong>，优先填满一个 prefill 请求中的 token，再继续处理下一个，在预算允许的情况下可连续处理多个 prefill 请求。<strong>若仍有剩余 token 预算，则进一步接纳新的 prefill 请求加入当前批次</strong>。</p><p><strong>系统会确保当前调度轮次中 decode 和 prefill 的 token 总数不超过预设的 chunk size。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507081058888.png" alt=""></p><blockquote><p>注：论文中提到了两个相关概念：</p><ul><li>一是 token budget，这个概念较为明确，用于决定每轮迭代中允许处理的最大 token 数（即 chunk size 的上限）；</li><li>二是 chunk size，其使用较为模糊，有时指代 token budget，有时又表示实际 chunk 中包含的 token 数量。</li></ul><p>在 Sarathi-Serve 的实现代码中，chunk_size 明确用于表示每轮迭代中 token 数的上限（即 token budget）。为了避免混淆，本文中所提到的 chunk size 均指 每轮迭代的 token 上限。</p></blockquote><p>除了<strong>固定的 chunk size</strong>（默认值是 512 个 token，该值是 Sarathi 论文中基于硬件特性和性能分析实验计算出的，在单个 batch 中能够最大化 GPU 计算饱和度的 token 数量。）之外，Sarathi-Serve 还提供了**动态 chunk size **机制，这是一种渐进式的分块策略，旨在平衡首 token 延迟和整体吞吐量。该机制将长提示词的处理过程划分为多个阶段，在早期阶段使用较大的 chunk size（如 2048 个 token）来快速推进处理，减少首 token 延迟；随着处理进度的推进，逐步减小 chunk size（最终降至 256 个 token），避免长提示词的后续处理阻塞其他请求。</p><p>固定 chunk size 是包含 prefill + decode token 的总数。例如，512 token 的 batch 可能包含：2 个 decode 请求（各 1 token）+ prefill 请求 1（400 个 token）+ prefill 请求 2（110 个 token）= 512 个 token。</p><p>而动态 chunk size 对于不同阶段的 prefill 请求是不一样的，比如 chunk_sizes 列表是 [1024, 512, 256]，一个 batch 可能包含 2 个 decode 请求（各 1 个 token）+ prefill 请求 1（250 个 token，阶段 3）+ prefill 请求 2（772 个 token，阶段 1，1024-2-250=772）= 1024 个 token。</p><h4 id="_2-3-1-stall-free-scheduling-和其他调度策略的对比" tabindex="-1">2.3.1 Stall-Free Scheduling 和其他调度策略的对比 <a class="header-anchor" href="#_2-3-1-stall-free-scheduling-和其他调度策略的对比" aria-label="Permalink to &quot;2.3.1 Stall-Free Scheduling 和其他调度策略的对比&quot;">​</a></h4><p>**在 Sarathi-Serve 的设计理念下，无论是 prefill 阶段还是 decode 阶段，都不会产生停滞（stall）。**从作者的观点来看，其余推理系统（如 vLLM、Orca、FasterTransformer）在调度策略上或多或少都牺牲了一方的性能以保全另一方：</p><p><strong>prefill 优先的调度策略（prefill-prioritized schedules）</strong>：</p><ul><li>vLLM 会优先调度尽可能多的 prefill 请求，只有在完成这些 prefill 后才恢复 decode，从而造成 decode 阶段的阻塞，导致 TBT 延迟上升。</li><li>Orca 和 vLLM 都采用 FCFS（先来先服务）的 iteration-level batching 策略，并同样优先处理 prefill 请求。但在 batch 组成策略上有所不同：vLLM 仅支持纯 prefill 或纯 decode 的 batch，而 Orca 支持 prefill 和 decode 的混合 batch。<strong>尽管如此，Orca 的混合 batch 在包含长 prompt 时执行时间依然较长，decode 阶段依旧受到影响，无法避免 decode 阻塞</strong>。</li></ul><p><strong>decode 优先的调度策略（decode-prioritized schedules）</strong>：</p><ul><li>FasterTransformer 采用 request-level batching 策略，在当前请求的 decode 阶段全部完成之前，不会调度任何新的请求。例如在下图中，请求 C 和 D 的 prefill 将被阻塞，直到请求 A 和 B 完全退出系统。<strong>该策略虽然可以显著降低 TBT 延迟</strong>，但也牺牲了系统整体吞吐量。</li></ul><p><strong>无阻塞（stall-free）的调度策略</strong>：</p><ul><li>Sarathi-Serve 同样支持 prefill 和 decode 的并行执行，但相比 Orca，<strong>它通过精细控制每个 batch 中 prefill token 的数量，确保 decode 几乎不受影响</strong>。与 FasterTransformer 相比，Sarathi 的 decode 时间只略有延长（把 Sarathi-Serve 的绿色块和 FasterTransformer 的红色块相比，可以发现绿色块只长了一点），却显著提升了吞吐量，<strong>实现了低延迟与高吞吐的兼得</strong>。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507120922906.png" alt=""></p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><blockquote><p>注：目前 vLLM 已支持 chunked-prefills 和混合 batch，以上关于 vLLM 的描述是基于论文撰写时的实现情况。</p></blockquote><h4 id="_2-3-2-混合-batch-的性能提升效果" tabindex="-1">2.3.2 混合 Batch 的性能提升效果 <a class="header-anchor" href="#_2-3-2-混合-batch-的性能提升效果" aria-label="Permalink to &quot;2.3.2 混合 Batch 的性能提升效果&quot;">​</a></h4><p>下图展示了在 A6000 GPU 上运行 LLaMA-13B 模型，不同 batch 组合方式下每个 token 的处理时间（单位：毫秒）：</p><ul><li>仅包含 prompt 的请求（prompt 长度为 1024，batch 大小为 4）；</li><li>仅包含 decode 的请求（batch 大小为 4，序列长度为 1024）；</li><li>一个混合 batch，包括 1 个长度为 1021 的 prefill 请求和 3 个 decode 请求。</li></ul><p><strong>结果表明，混合 batch 能将每个 token 的解码时间显著降低一个数量级，大幅提升整体推理效率；同时，prefill 阶段的耗时几乎没有变化。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061626435.png" alt=""></p><p><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">图片来源：SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills</a></p><h4 id="_2-3-3-chunked-prefills-的开销" tabindex="-1">2.3.3 Chunked-Prefills 的开销 <a class="header-anchor" href="#_2-3-3-chunked-prefills-的开销" aria-label="Permalink to &quot;2.3.3 Chunked-Prefills 的开销&quot;">​</a></h4><p>chunked-prefills 的开销主要来自两个方面：</p><ul><li>第一，<strong>当 chunk size 变小时，chunked-prefills 的算术强度会下降，进而降低 GPU 利用率，从而影响 prefill 阶段的效率</strong>。下图展示了在 Yi-34B 模型中，chunking 操作对整体 prefill 时延带来的影响。如预期所示，chunk 的划分越细（例如 size 为 512），带来的开销越大；<strong>但整体来看，开销的增长始终控制在 1.25 倍以内，属于可接受的范围</strong>。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061648369.png" alt=""></p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><ul><li>第二，<strong>chunked-prefills 会对 Attention 计算造成轻微开销，因为每个 chunk 在执行 Attention 时需要重复从 GPU 内存中读取该请求之前所有 chunk 的 KV cache</strong>。如下图所示，在 prompt 的前向传递结束前，所有 chunked-prefills 操作的 FFN 计算量是相同的，但从第二个 chunk 开始，每一个 Attention kernel 都必须重新读取之前所有 token 的 KV 对。例如，如果将一个 prefill 序列切分为 N 个 chunk，则第一个 chunk 的 KV cache 会被读取 N 次，第二个读取 N−1 次，依此类推。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061642730.png" alt=""></p><p><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">图片来源：SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills</a></p><p>尽管这种额外的 Attention 计算时间带来了一定开销，但由于 Attention 在整个前向传递中所占比例较小（如下图所示），因此它对端到端 prefill 效率的影响并不显著。</p><p>下图将 prefill 和 decode 阶段的计算时间细分为线性操作（linear）、注意力机制（Attention）以及其他部分，并展示了它们各自的耗时占比。可以看出，线性操作占据了绝大部分的执行时间。尽管 Attention 的开销会随序列长度呈平方增长，但即使在较长的序列下，<strong>线性操作仍占据超过 80% 的总耗时</strong>。因此，优化线性操作对于提升大模型推理效率至关重要。<strong>而我们前面提到，像 preproj、postproj、FFN1 和 FFN2 这类线性操作，恰恰是可以通过批处理来提升效率的。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507061215253.png" alt=""></p><p><a href="https://arxiv.org/abs/2403.02310" target="_blank" rel="noreferrer">图片来源：Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve</a></p><h4 id="_2-3-4-如何确定最佳的-chunk-size" tabindex="-1">2.3.4 如何确定最佳的 Chunk Size <a class="header-anchor" href="#_2-3-4-如何确定最佳的-chunk-size" aria-label="Permalink to &quot;2.3.4 如何确定最佳的 Chunk Size&quot;">​</a></h4><p><strong>在延迟（TBT）目标与 prefill 开销之间寻求平衡</strong></p><p>较小的 chunk size 有助于减少 TBT 延迟，因为每轮 iteration 涉及的 prefill token 更少，执行速度更快。</p><p>但如果 chunk size 过小，也会带来一系列问题：</p><ul><li>每个 chunk 的 Attention 操作都需重复读取此前的 KV cache，增加内存访问负担；</li><li>算术强度下降，GPU 利用率降低；</li><li>kernel 启动的固定开销更频繁，影响整体效率。</li></ul><p><strong>因此，在确定 chunk size 时，需要在 prefill 的计算开销与 decode 的延迟之间做出合理权衡</strong>。可以通过一次性对不同 token 数量的 batch 进行 profiling，<strong>找出在不违反 TBT SLO 的前提下，单个 batch 可容纳的最大 token 数</strong>，从而设定合适的 chunk size。论文中借助工具 <a href="https://github.com/microsoft/vidur" target="_blank" rel="noreferrer">Vidur</a> 自动化完成这一过程，确保最终配置既能最大化吞吐量，又能有效控制延迟。</p><p><strong>避免 tile quantization 效应</strong></p><p>GPU 执行矩阵乘法时通常采用 tile 分块机制（例如 tile size = 128），只有当矩阵维度是 tile 的整数倍时，资源利用率才最高。</p><p>如果 chunk size 刚好超过 tile size 的倍数（例如 257），就会导致 thread blocks 内部部分线程空闲或执行无效计算，即“空转”，从而引发突发性的计算时间激增。下图展示了这一现象：<strong>当序列长度从 256 增加到 257，仅增加 1 个 token，延迟却从 69.8ms 飙升至 92.33ms，涨幅高达 32%</strong>。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507122057905.png" alt=""></p><p><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noreferrer">图片来源：SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked-Prefills</a></p><p>当序列长度恰好是 tile size（128）的整数倍时，如 128、256、384 等，运行时间上升相对平稳；而一旦略微超过 tile 边界（例如从 256 到 257），计算时间则会急剧增加。</p><p>这是因为 GPU 的矩阵乘法是按 tile 并行执行的，如果维度不是 tile 的整数倍，部分 tile 无法充分利用，导致计算资源浪费，这就是所谓的 tile quantization overhead。</p><p>为避免这种问题，推荐的做法是：<strong>选择合适的 chunk size，并使其与搭载（piggyback） 的 decode token 数之和是 tile size 的整数倍，从而保持矩阵维度对齐，确保计算效率最优</strong>。</p><h2 id="_3-vllm-中设置-chunked-prefills" tabindex="-1">3 vLLM 中设置 Chunked-Prefills <a class="header-anchor" href="#_3-vllm-中设置-chunked-prefills" aria-label="Permalink to &quot;3 vLLM 中设置 Chunked-Prefills&quot;">​</a></h2><p>在 vLLM v1 中，chunked-prefills 是默认启用的。在 vLLM 中可以通过调整 <code>max_num_batched_tokens</code> 参数来优化性能：</p><ul><li>设置较小的值（例如 2048）可以提升 <strong>token 间延迟（ITL，Inter-Token Latency， 也就是 TPOT）</strong>，因为每轮调度中 prefill token 更少，不会拖慢 decode 的执行；</li><li>设置较大的值可以提升<strong>首 token 响应时间（TTFT，Time To First Token）</strong>，因为每轮可以处理更多的 prefill token；</li><li>如果追求最佳吞吐量，<strong>建议将 <code>max_num_batched_tokens</code> 设置大于 8096</strong>，特别是在使用小模型和大显存 GPU 的场景下。</li></ul><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> vllm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> LLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># Set max_num_batched_tokens to tune performance</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">llm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> LLM(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;meta-llama/Llama-3.1-8B-Instruct&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">max_num_batched_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">16384</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><p>你可以使用 <a href="https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_throughput.py" target="_blank" rel="noreferrer">benchmark_throughput.py</a> 脚本，在你的 GPU 服务器上测试不同参数组合，从而找到最优配置。</p><p>可以执行以下命令安装 vLLM 并下载测试数据集：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装 uv，管理 python 虚拟环境</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -LsSf</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://astral.sh/uv/install.sh</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> sh</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">source</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $HOME</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/.local/bin/env</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装 GPU Driver</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://cn.download.nvidia.com/tesla/565.57.01/NVIDIA-Linux-x86_64-565.57.01.run</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sh</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NVIDIA-Linux-x86_64-565.57.01.run</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --silent</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装 CUDA Toolkit（如 nvcc、include、lib64）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -y</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> nvidia-cuda-toolkit</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 创建 python 虚拟环境</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">uv</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> venv</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vllm-demo</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --python</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 3.12</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --seed</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">source</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vllm-demo/bin/activate</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装 vLLM</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">uv</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vllm</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装 benchmark 所需依赖</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">uv</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pandas</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> datasets</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 下载测试数据集</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://github.com/vllm-project/vllm.git</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vllm/benchmark</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json</span></span></code></pre></div><p><code>benchmark_throughput.py</code> 的主要参数如下：</p><ul><li><code>--model</code>：指定要测试的模型。</li><li><code>--dataset</code>：指定测试数据集。</li><li><code>--max-model-len</code>：设置单个请求的最大序列长度（默认值: 使用模型配置中的值，比如 deepseek-ai/DeepSeek-R1-Distill-Llama-8B 模型中这个参数的值是 131072，这个长度至少需要 16.00 GiB 的显存用于存储 KV cache，如果 GPU 显存不够可以调小这个参数）。当输入提示长度超过该值时，vLLM 会抛出错误并拒绝处理该请求。</li><li><code>--max-num-batched-tokens</code>：设置单次迭代中处理的最大 token 总数（默认值: 对于启用分块预填充的情况为 2048，否则为 max(max_model_len, 2048)）。<code>max-num-batched-tokens</code> 也就是 token budget，决定了单次迭代中 chunk size 的上限，而实际的 chunk size 会根据当前批处理中的其他序列和剩余的 token budget 动态计算。</li><li><code>--num-prompts</code>：指定基准测试中要处理的 promt 数量。（默认值: 1000）</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># vLLM v1 中默认开启 chunked-prefills，可以通过 --no-enable-chunked-prefill 参数禁用</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> benchmark_throughput.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--backend </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">vllm</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--model </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;deepseek-ai/DeepSeek-R1-Distill-Llama-8B&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--dataset </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">ShareGPT_V3_unfiltered_cleaned_split.json</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--max-model-len </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">8192</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--max-num-batched-tokens </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">2048</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--num-prompts </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">1000</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 输出结果</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Throughput:</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 4.85</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> requests/s,</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 2007.66</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> total</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tokens/s,</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 962.92</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> output</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tokens/s</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Total</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> num</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> prompt</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tokens:</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  215196</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Total</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> num</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> output</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tokens:</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  198343</span></span></code></pre></div><h2 id="_4-总结" tabindex="-1">4 总结 <a class="header-anchor" href="#_4-总结" aria-label="Permalink to &quot;4 总结&quot;">​</a></h2><p>文章介绍了大模型推理中 prefill 与 decode 阶段在资源利用上的差异所带来的调度挑战，并回顾了从 Static Batching 到 Continuous Batching 的策略演进。为解决传统静态或迭代调度中存在的资源浪费与延迟问题，Sarathi-Serve 提出了 chunked-prefills 和 stall-free scheduling 机制，通过将长 prompt 拆分为多个小块，并与 decode 请求混合调度，从而实现高吞吐与低延迟的平衡。</p><h2 id="_5-附录" tabindex="-1">5 附录 <a class="header-anchor" href="#_5-附录" aria-label="Permalink to &quot;5 附录&quot;">​</a></h2><h3 id="_5-1-推理中衡量延迟的几个术语" tabindex="-1">5.1 推理中衡量延迟的几个术语 <a class="header-anchor" href="#_5-1-推理中衡量延迟的几个术语" aria-label="Permalink to &quot;5.1 推理中衡量延迟的几个术语&quot;">​</a></h3><ul><li><strong>TTFT</strong>（Time To First Token）：指从用户发出请求到模型生成第一个 token 所花费的时间，用于衡量 prefill 阶段的性能。</li><li><strong>TBT</strong>（Time Between Tokens）：指连续生成两个 token 之间所花费的时间，反映每个 token 的生成速度。</li><li><strong>TPOT</strong>（Time Per Output Token）：指所有输出 token 的平均生成时间，有时也称为 ITL（Inter-Token Latency），反映整体生成效率。</li></ul><p>整体响应延迟可用以下公式计算：</p>`,96)),t("mjx-container",y,[(i(),l("svg",H,e[15]||(e[15]=[s('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" style="stroke-width:3;"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(625,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(1125,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(1514,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1958,0)" style="stroke-width:3;"></path><path data-c="63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z" transform="translate(2514,0)" style="stroke-width:3;"></path><path data-c="79" d="M69 -66Q91 -66 104 -80T118 -116Q118 -134 109 -145T91 -160Q84 -163 97 -166Q104 -168 111 -168Q131 -168 148 -159T175 -138T197 -106T213 -75T225 -43L242 0L170 183Q150 233 125 297Q101 358 96 368T80 381Q79 382 78 382Q66 385 34 385H19V431H26L46 430Q65 430 88 429T122 428Q129 428 142 428T171 429T200 430T224 430L233 431H241V385H232Q183 385 185 366L286 112Q286 113 332 227L376 341V350Q376 365 366 373T348 383T334 385H331V431H337H344Q351 431 361 431T382 430T405 429T422 429Q477 429 503 431H508V385H497Q441 380 422 345Q420 343 378 235T289 9T227 -131Q180 -204 113 -204Q69 -204 44 -177T19 -116Q19 -89 35 -78T69 -66Z" transform="translate(2958,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3763.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(4819.6,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" style="stroke-width:3;"></path><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" transform="translate(722,0)" style="stroke-width:3;"></path><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(1444,0)" style="stroke-width:3;"></path><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" transform="translate(2097,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7860.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8861,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(9250,0)"><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" style="stroke-width:3;"></path><path data-c="50" d="M130 622Q123 629 119 631T103 634T60 637H27V683H214Q237 683 276 683T331 684Q419 684 471 671T567 616Q624 563 624 489Q624 421 573 372T451 307Q429 302 328 301H234V181Q234 62 237 58Q245 47 304 46H337V0H326Q305 3 182 3Q47 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM507 488Q507 514 506 528T500 564T483 597T450 620T397 635Q385 637 307 637H286Q237 637 234 628Q231 624 231 483V342H302H339Q390 342 423 349T481 382Q507 411 507 488Z" transform="translate(722,0)" style="stroke-width:3;"></path><path data-c="4F" d="M56 340Q56 423 86 494T164 610T270 680T388 705Q521 705 621 601T722 341Q722 260 693 191T617 75T510 4T388 -22T267 3T160 74T85 189T56 340ZM467 647Q426 665 388 665Q360 665 331 654T269 620T213 549T179 439Q174 411 174 354Q174 144 277 61Q327 20 385 20H389H391Q474 20 537 99Q603 188 603 354Q603 411 598 439Q577 592 467 647Z" transform="translate(1403,0)" style="stroke-width:3;"></path><path data-c="54" d="M36 443Q37 448 46 558T55 671V677H666V671Q667 666 676 556T685 443V437H645V443Q645 445 642 478T631 544T610 593Q593 614 555 625Q534 630 478 630H451H443Q417 630 414 618Q413 616 413 339V63Q420 53 439 50T528 46H558V0H545L361 3Q186 1 177 0H164V46H194Q264 46 283 49T309 63V339V550Q309 620 304 625T271 630H244H224Q154 630 119 601Q101 585 93 554T81 486T76 443V437H36V443Z" transform="translate(2181,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(12375.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(13375.4,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">生</text><text data-variant="normal" transform="translate(1000,0) scale(1,-1)" font-size="884px" font-family="serif">成</text><path data-c="20" d="" transform="translate(2000,0)" style="stroke-width:3;"></path><path data-c="74" d="M27 422Q80 426 109 478T141 600V615H181V431H316V385H181V241Q182 116 182 100T189 68Q203 29 238 29Q282 29 292 100Q293 108 293 146V181H333V146V134Q333 57 291 17Q264 -10 221 -10Q187 -10 162 2T124 33T105 68T98 100Q97 107 97 248V385H18V422H27Z" transform="translate(2250,0)" style="stroke-width:3;"></path><path data-c="6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z" transform="translate(2639,0)" style="stroke-width:3;"></path><path data-c="6B" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T97 124T98 167T98 217T98 272T98 329Q98 366 98 407T98 482T98 542T97 586T97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V463L180 233L240 287Q300 341 304 347Q310 356 310 364Q310 383 289 385H284V431H293Q308 428 412 428Q475 428 484 431H489V385H476Q407 380 360 341Q286 278 286 274Q286 273 349 181T420 79Q434 60 451 53T500 46H511V0H505Q496 3 418 3Q322 3 307 0H299V46H306Q330 48 330 65Q330 72 326 79Q323 84 276 153T228 222L176 176V120V84Q176 65 178 59T189 49Q210 46 238 46H254V0H246Q231 3 137 3T28 0H20V46H36Z" transform="translate(3139,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(3667,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(4111,0)" style="stroke-width:3;"></path><path data-c="20" d="" transform="translate(4667,0)" style="stroke-width:3;"></path><text data-variant="normal" transform="translate(4917,0) scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mo" transform="translate(19292.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),e[16]||(e[16]=t("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[t("mtext",null,"Latency"),t("mo",null,"="),t("mtext",null,"TTFT"),t("mo",null,"+"),t("mo",{stretchy:"false"},"("),t("mtext",null,"TPOT"),t("mo",null,"×"),t("mtext",null,"生成 token 数"),t("mo",{stretchy:"false"},")")])],-1))]),e[20]||(e[20]=s(`<p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507131705925.png" alt=""></p><h3 id="_5-2-batch-机制" tabindex="-1">5.2 Batch 机制 <a class="header-anchor" href="#_5-2-batch-机制" aria-label="Permalink to &quot;5.2 Batch 机制&quot;">​</a></h3><p>在经典的批处理机制中，每次迭代时，Transformer 层会接收一个形状为 <code>[B, L, H]</code> 的三维输入张量，该张量是通过将一个 batch 中多个请求的 <code>[L, H]</code> 输入张量拼接而成的。其中：</p><ul><li><code>B</code> 表示 batch 大小（请求数量）。</li><li><code>L</code> 表示每个请求中一起处理的 token 数。</li><li><code>H</code> 表示模型的隐藏层维度（hidden size）。</li></ul><p>假设我们有一个 batch，里面包含 2 个请求，每个请求当前都需要处理 3 个 token，模型的 hidden size 为 10。那么这两个请求的输入张量分别是：</p><p>请求 1 的输入张量 <code>[3, 10]</code>：</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span>[</span></span>
<span class="line"><span>  [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],   ← 第 1 个 token</span></span>
<span class="line"><span>  [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],   ← 第 2 个 token</span></span>
<span class="line"><span>  [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0]    ← 第 3 个 token</span></span>
<span class="line"><span>]</span></span></code></pre></div><p>请求 2 的输入张量 <code>[3, 10]</code>：</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span>[</span></span>
<span class="line"><span>  [0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1],</span></span>
<span class="line"><span>  [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],</span></span>
<span class="line"><span>  [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]</span></span>
<span class="line"><span>]</span></span></code></pre></div><p>合并后，整体输入张量为 <code>[2, 3, 10]</code>：</p><div class="language-plaintext vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">plaintext</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span>[</span></span>
<span class="line"><span>  [   ← 请求 1</span></span>
<span class="line"><span>    [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],</span></span>
<span class="line"><span>    [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0],</span></span>
<span class="line"><span>    [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0]</span></span>
<span class="line"><span>  ],</span></span>
<span class="line"><span>  [   ← 请求 2</span></span>
<span class="line"><span>    [0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1],</span></span>
<span class="line"><span>    [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],</span></span>
<span class="line"><span>    [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]</span></span>
<span class="line"><span>  ]</span></span>
<span class="line"><span>]</span></span></code></pre></div><p>这个 <code>[B=2, L=3, H=10]</code> 的张量就可以作为 Transformer 一层的输入参与批处理计算。</p><h3 id="_5-3-张量并行和流水线并行" tabindex="-1">5.3 张量并行和流水线并行 <a class="header-anchor" href="#_5-3-张量并行和流水线并行" aria-label="Permalink to &quot;5.3 张量并行和流水线并行&quot;">​</a></h3><p>随着大语言模型（LLM）规模不断扩大，推理时必须跨多张 GPU 或多节点部署。为了解决单张 GPU 显存不足、单节点容量受限的问题，<strong>模型并行（Model Parallelism）</strong> 成为必要手段。而流水线并行（PP）是当前跨节点部署模型的主流方式，其核心优势是通信开销小、扩展性好。</p><p>在模型并行的常见方式中：</p><ul><li><strong>张量并行（Tensor Parallelism，TP）</strong> 将每一层的权重切分到多个 GPU 上，各 GPU 协同完成每一层的计算，KV cache 也被均匀分配。但这种方式每层都需要执行两次 All Reduce 操作（Attention 和 FFN 各一次），通信量大、延迟高，只适合在同节点内、如 NVLink 连接的高带宽环境下使用。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507131810335.png" alt=""></p><ul><li><strong>流水线并行（Pipeline Parallelism，PP）</strong> 则将整个模型按层切分，每个 GPU 负责一部分连续的层，多个 micro-batch 像“流水线”一样依次通过每个 GPU。相较 TP，PP 只需要在层与层之间做一次激活值的通信，通信开销更小，尤其适合集群间带宽有限的环境。PP 的另一个优势是能释放部分 GPU 显存，支持更大的 batch size，从而提升 decode 阶段的吞吐效率。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202507131811658.png" alt=""></p><p>因此，在缺乏高速互联（如 NVLink）的跨节点部署中，PP 是唯一可行且高效的模型并行方式，能将每节点的最大 batch size 提高 2–3 倍，大幅增强推理吞吐。</p><h3 id="_5-4-micro-batch-微批" tabindex="-1">5.4 micro-batch 微批 <a class="header-anchor" href="#_5-4-micro-batch-微批" aria-label="Permalink to &quot;5.4 micro-batch 微批&quot;">​</a></h3><p><strong>micro-batch（微批）</strong> 是将一个完整的 batch 拆分成多个更小的子批次，用于提升硬件资源利用率，尤其在<strong>流水线并行（Pipeline Parallelism）</strong> 中非常常见。</p><p>当大模型被划分为多个阶段并分布在不同 GPU 上时，如果直接处理整个 batch，会导致部分 GPU 处于空闲等待状态。为了解决这个问题，我们将 batch 拆分成多个 micro-batch，并让它们像“流水线”一样在各阶段依次推进。这样，每个阶段的 GPU 都可以同时处理不同的 micro-batch，大幅提高并行度和吞吐量，减少资源浪费。</p><p>举个例子，如果一个 batch 有 64 个样本，可以被拆成 8 个 micro-batch，每个包含 8 个样本，在模型各阶段中交错处理，从而避免 GPU 空转，提高执行效率。每个阶段表示模型中一部分连续的层，由一个 GPU 负责计算。例如，在一个 12 层的 Transformer 模型中，若使用 4 个 GPU，则每个阶段可能包含 3 层。</p><h2 id="_6-参考资料" tabindex="-1">6 参考资料 <a class="header-anchor" href="#_6-参考资料" aria-label="Permalink to &quot;6 参考资料&quot;">​</a></h2><ul><li>图解大模型计算加速系列：分离式推理架构2，模糊分离与合并边界的Chunked-Prefills：<a href="https://zhuanlan.zhihu.com/p/710165390" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/710165390</a></li><li>大模型推理核心技术之Continuous Batching和我的WXG往事：<a href="https://zhuanlan.zhihu.com/p/676109470" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/676109470</a></li><li>How continuous batching enables 23x throughput in LLM inference while reducing p50 latency：<a href="https://www.anyscale.com/blog/continuous-batching-llm-inference" target="_blank" rel="noreferrer">https://www.anyscale.com/blog/continuous-batching-llm-inference</a></li><li>vllm调度笔记：chunked prefill调度策略：<a href="https://zhuanlan.zhihu.com/p/711209924" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/711209924</a></li><li>vLLM调度器解密（下）：chunked prefill是如何进一步优化的？：<a href="https://zhuanlan.zhihu.com/p/6144374775" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/6144374775</a></li><li>vLLM Optimization and Tuning：<a href="https://docs.vllm.ai/en/latest/configuration/optimization.html" target="_blank" rel="noreferrer">https://docs.vllm.ai/en/latest/configuration/optimization.html</a></li><li>[EXTERNAL] OSS Chunked Prefill Evaluation：<a href="https://docs.google.com/document/d/1W6t6wouQKgl1QivS7gbkkY5xtR9Q6wvXbmOL-1onMmk/edit?tab=t.0#heading=h.8um4c511b0b0" target="_blank" rel="noreferrer">https://docs.google.com/document/d/1W6t6wouQKgl1QivS7gbkkY5xtR9Q6wvXbmOL-1onMmk/edit?tab=t.0#heading=h.8um4c511b0b0</a></li><li>[RFC] Upstream Chunked Prefill：<a href="https://github.com/vllm-project/vllm/issues/3130" target="_blank" rel="noreferrer">https://github.com/vllm-project/vllm/issues/3130</a></li><li>NVIDIA NIM LLMs Benchmarking：<a href="https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html#inter-token-latency-itl" target="_blank" rel="noreferrer">https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html#inter-token-latency-itl</a></li><li>Matrix-vector and Matrix-matrix Multiplication：<a href="https://www.youtube.com/watch?v=7CBkZq3eQ_0" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=7CBkZq3eQ_0</a></li><li>Paradigms of Parallelism：<a href="https://colossalai.org/docs/concepts/paradigms_of_parallelism" target="_blank" rel="noreferrer">https://colossalai.org/docs/concepts/paradigms_of_parallelism</a></li></ul><h2 id="欢迎关注" tabindex="-1">欢迎关注 <a class="header-anchor" href="#欢迎关注" aria-label="Permalink to &quot;欢迎关注&quot;">​</a></h2><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202503222156941.png" alt=""></p>`,28))])}const B=d(m,[["render",F]]);export{A as __pageData,B as default};
