<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>PD 分离推理架构详解 | Se7en的架构笔记</title>
    <meta name="description" content="个人技术知识库，记录 & 分享个人碎片化、结构化、体系化的技术知识内容。">
    <meta name="generator" content="VitePress v1.0.0-rc.31">
    <link rel="preload stylesheet" href="/assets/style.CsloOiH0.css" as="style">
    
    <script type="module" src="/assets/app.B4GI1vF8.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Bu8hRsVA.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.DIkCQIk8.js">
    <link rel="modulepreload" href="/assets/chunks/theme.CaMdk3Oi.js">
    <link rel="modulepreload" href="/assets/chunks/md5.BFEskVOY.js">
    <link rel="modulepreload" href="/assets/chunks/use-popup-manager.BHZe4FEH.js">
    <link rel="modulepreload" href="/assets/chunks/DirectoryList._vTsPPQD.js">
    <link rel="modulepreload" href="/assets/chunks/ArticleMetadata.Cxlq8Gj7.js">
    <link rel="modulepreload" href="/assets/courses_ai-infra_AI Infra 教程_06-disaggregating-prefill-and-decoding.md.CJsgR_Ns.lean.js">
    <link rel="icon" href="/favicon.ico">
    <meta name="author" content="Se7en">
    <meta name="keywords" content="Se7en的架构笔记, 知识库, 博客, Se7en">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="theme-color" content="#3c8772">
    <meta property="og:type" content="website">
    <meta property="og:locale" content="zh_CN">
    <meta property="og:title" content="Se7en的架构笔记">
    <meta property="og:description" content="个人技术知识库，记录 &amp; 分享个人碎片化、结构化、体系化的技术知识内容。">
    <meta property="og:site" content="https://blog.charles7c.top">
    <meta property="og:site_name" content="Se7en的架构笔记">
    <meta property="og:image" content="https://blog.charles7c.top/logo.jpg">
    <script>var _hmt=_hmt||[];(function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?53af4b1a12fbe40810ca7ad39f8db9c7";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)})();</script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a060c8de><!--[--><!--]--><!--[--><span tabindex="-1" data-v-2ef2e661></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-2ef2e661> Skip to content </a><!--]--><!----><header class="VPNav" data-v-a060c8de data-v-6f464c6b><div class="VPNavBar" data-v-6f464c6b data-v-805827ca><div class="container" data-v-805827ca><div class="title" data-v-805827ca><div class="VPNavBarTitle has-sidebar" data-v-805827ca data-v-904f8210><a class="title" href="/" data-v-904f8210><!--[--><!--]--><!--[--><img class="VPImage logo" src="/logo.png" alt data-v-cc4cfae8><!--]--><!--[-->Se7en的架构笔记<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-805827ca><div class="curtain" data-v-805827ca></div><div class="content-body" data-v-805827ca><!--[--><!--]--><div class="VPNavBarSearch search" data-v-805827ca><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-805827ca data-v-ffa2482b><span id="main-nav-aria-label" class="visually-hidden" data-v-ffa2482b>Main Navigation</span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-ffa2482b data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-45d18c93><span class="text" data-v-45d18c93><!----><span data-v-45d18c93>博客</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-45d18c93><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><div class="items" data-v-d8f08df7><!--[--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/blogs/original/index" data-v-f4a5317a><!--[-->原创<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/blogs/translate/index" data-v-f4a5317a><!--[-->翻译<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-ffa2482b data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-45d18c93><span class="text" data-v-45d18c93><!----><span data-v-45d18c93>我的分类</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-45d18c93><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><div class="items" data-v-d8f08df7><!--[--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/categories/issues/index" data-v-f4a5317a><!--[-->Bug万象集<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/categories/fragments/index" data-v-f4a5317a><!--[-->个人速查手册<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/categories/tools/index" data-v-f4a5317a><!--[-->精选工具箱<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/categories/open-source/index" data-v-f4a5317a><!--[-->开源项目<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/categories/learning/index" data-v-f4a5317a><!--[-->学习笔记<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-ffa2482b data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-45d18c93><span class="text" data-v-45d18c93><!----><span data-v-45d18c93>我的小册</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-45d18c93><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><div class="items" data-v-d8f08df7><!--[--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link active" href="/courses/ai-infra/index" data-v-f4a5317a><!--[-->AI Infra 教程<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/courses/elastic-stack/index" data-v-f4a5317a><!--[-->Elastic Stack 实战教程<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/courses/observability/index" data-v-f4a5317a><!--[-->Observability 教程<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/courses/interview/index" data-v-f4a5317a><!--[-->面试宝典<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/courses/algorithm/index" data-v-f4a5317a><!--[-->数据结构与算法<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/tags" tabindex="0" data-v-ffa2482b data-v-4a0e8c7c><!--[--><span data-v-4a0e8c7c>我的标签</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/archives" tabindex="0" data-v-ffa2482b data-v-4a0e8c7c><!--[--><span data-v-4a0e8c7c>我的归档</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-ffa2482b data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-45d18c93><span class="text" data-v-45d18c93><!----><span data-v-45d18c93>关于</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-45d18c93><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><div class="items" data-v-d8f08df7><!--[--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/about/index" data-v-f4a5317a><!--[-->关于知识库<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-d8f08df7 data-v-f4a5317a><a class="VPLink link" href="/about/me" data-v-f4a5317a><!--[-->关于我<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-805827ca data-v-aaadbb3e data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="Change language" data-v-45d18c93><span class="text" data-v-45d18c93><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-45d18c93><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-45d18c93><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><!----><!--[--><!--[--><div class="items" data-v-aaadbb3e><p class="title" data-v-aaadbb3e>中文</p><!--[--><div class="VPMenuLink" data-v-aaadbb3e data-v-f4a5317a><a class="VPLink link" href="/en/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/06-disaggregating-prefill-and-decoding" data-v-f4a5317a><!--[-->English<!--]--></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-805827ca data-v-5e47bad3><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-5e47bad3 data-v-9179508e data-v-724bbf4f><span class="check" data-v-724bbf4f><span class="icon" data-v-724bbf4f><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-9179508e><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-9179508e><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-805827ca data-v-240f015e data-v-4da44f34><!--[--><a class="VPSocialLink no-icon" href="https://github.com/cr7258/cr7258.github.io" aria-label="github" target="_blank" rel="noopener" data-v-4da44f34 data-v-27377fe2><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-805827ca data-v-390b231e data-v-45d18c93><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-45d18c93><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-45d18c93><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-45d18c93><div class="VPMenu" data-v-45d18c93 data-v-d8f08df7><!----><!--[--><!--[--><div class="group translations" data-v-390b231e><p class="trans-title" data-v-390b231e>中文</p><!--[--><div class="VPMenuLink" data-v-390b231e data-v-f4a5317a><a class="VPLink link" href="/en/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/06-disaggregating-prefill-and-decoding" data-v-f4a5317a><!--[-->English<!--]--></a></div><!--]--></div><div class="group" data-v-390b231e><div class="item appearance" data-v-390b231e><p class="label" data-v-390b231e>切换日光/暗黑模式</p><div class="appearance-action" data-v-390b231e><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to dark theme" aria-checked="false" data-v-390b231e data-v-9179508e data-v-724bbf4f><span class="check" data-v-724bbf4f><span class="icon" data-v-724bbf4f><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-9179508e><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-9179508e><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-390b231e><div class="item social-links" data-v-390b231e><div class="VPSocialLinks social-links-list" data-v-390b231e data-v-4da44f34><!--[--><a class="VPSocialLink no-icon" href="https://github.com/cr7258/cr7258.github.io" aria-label="github" target="_blank" rel="noopener" data-v-4da44f34 data-v-27377fe2><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-805827ca data-v-e0a65837><span class="container" data-v-e0a65837><span class="top" data-v-e0a65837></span><span class="middle" data-v-e0a65837></span><span class="bottom" data-v-e0a65837></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav reached-top" data-v-a060c8de data-v-048da602><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-048da602><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-048da602><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-048da602>文章</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-048da602 data-v-975b542f><button data-v-975b542f>返回顶部</button><!----></div></div><aside class="VPSidebar" data-v-a060c8de data-v-76a1a689><div class="curtain" data-v-76a1a689></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-76a1a689><span class="visually-hidden" id="sidebar-aria-label" data-v-76a1a689> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-76a1a689><section class="VPSidebarItem level-0 collapsible has-active" data-v-76a1a689 data-v-97efc1f4><div class="item" role="button" tabindex="0" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><h2 class="text" data-v-97efc1f4>AI Infra 教程 (6篇)</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-97efc1f4><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="caret-icon" data-v-97efc1f4><path d="M9,19c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l5.3-5.3L8.3,6.7c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l6,6c0.4,0.4,0.4,1,0,1.4l-6,6C9.5,18.9,9.3,19,9,19z"></path></svg></div></div><div class="items" data-v-97efc1f4><!--[--><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/01-vllm-quickstart" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>1. vLLM 快速部署指南</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/02-pagedattention" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>2. vLLM 核心技术 PagedAttention 原理详解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/03-prefix-caching" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>3. Prefix Caching 详解：实现 KV Cache 的跨请求高效复用</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/04-speculative-decoding" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>4. Speculative Decoding 推测解码方案详解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/05-chunked-prefills" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>5. Chunked Prefills 分块预填充详解</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-97efc1f4 data-v-97efc1f4><div class="item" data-v-97efc1f4><div class="indicator" data-v-97efc1f4></div><a class="VPLink link link" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/06-disaggregating-prefill-and-decoding" data-v-97efc1f4><!--[--><p class="text" data-v-97efc1f4>6. PD 分离推理架构详解</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a060c8de data-v-ecabc458><div class="VPDoc has-sidebar has-aside" data-v-ecabc458 data-v-47739f1d><!--[--><!--]--><div class="container" data-v-47739f1d><div class="aside" data-v-47739f1d><div class="aside-curtain" data-v-47739f1d></div><div class="aside-container" data-v-47739f1d><div class="aside-content" data-v-47739f1d><div class="VPDocAside" data-v-47739f1d data-v-fa12ad52><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-fa12ad52 data-v-1320f35f><div class="content" data-v-1320f35f><div class="outline-marker" data-v-1320f35f></div><div class="outline-title" role="heading" aria-level="2" data-v-1320f35f>目录</div><nav aria-labelledby="doc-outline-aria-label" data-v-1320f35f><span class="visually-hidden" id="doc-outline-aria-label" data-v-1320f35f> Table of Contents for current page </span><ul class="root" data-v-1320f35f data-v-6decdfa4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-fa12ad52></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-47739f1d><div class="content-container" data-v-47739f1d><!--[--><!--]--><!----><main class="main" data-v-47739f1d><div style="position:relative;" class="vp-doc _courses_ai-infra_AI%20Infra%20%E6%95%99%E7%A8%8B_06-disaggregating-prefill-and-decoding" data-v-47739f1d><div><h1 id="pd-分离推理架构详解" tabindex="-1">PD 分离推理架构详解 <a class="header-anchor" href="#pd-分离推理架构详解" aria-label="Permalink to &quot;PD 分离推理架构详解&quot;">​</a></h1><!----><p>PD 分离推理架构的讲解视频可以在这里观看：<a href="https://www.bilibili.com/video/BV1ZTWAzmEEc" target="_blank" rel="noreferrer">https://www.bilibili.com/video/BV1ZTWAzmEEc</a></p><p>本文是 LLM 推理系列的第 6 篇，介绍 PD 分离推理架构。</p><p>往期文章：</p><ul><li><a href="https://mp.weixin.qq.com/s/rVW6jjLQabHGMMwnbIzB7Q" target="_blank" rel="noreferrer">vLLM 快速部署指南</a></li><li><a href="https://mp.weixin.qq.com/s/94-kEyHui0BLO5S-80eAiw" target="_blank" rel="noreferrer">vLLM 核心技术 PagedAttention 原理详解</a></li><li><a href="https://mp.weixin.qq.com/s/_FnXC7hiQtwyzU-ISvU0CA" target="_blank" rel="noreferrer">Prefix Caching 详解：实现 KV Cache 的跨请求高效复用</a></li><li><a href="https://mp.weixin.qq.com/s/sdIt8PpZDZ8DB8iKJ4xoEA" target="_blank" rel="noreferrer">Speculative Decoding 推测解码方案详解</a></li><li><a href="https://mp.weixin.qq.com/s/JIvbYapMMtC8JkBEXHsG-A" target="_blank" rel="noreferrer">Chunked-Prefills 分块预填充机制详解</a></li></ul><p>在大语言模型推理过程中，prefill 阶段和 decode 阶段具有截然不同的计算特性：</p><ul><li>prefill 阶段需要并行处理整个输入序列来生成首个 token，属于计算密集型操作。</li><li>decode 阶段则逐个生成后续 token，需要频繁访问 KV cache，属于内存密集型操作。</li></ul><p>传统的 continuous batching 将两个阶段混合处理，导致相互干扰，难以同时满足 TTFT（首 token 延迟）和 TPOT（token 间延迟）的严格要求。为了解决这一问题，PD 分离架构应运而生，通过将 prefill 和 decode 分配到不同的 GPU 实例上，针对各自特性进行专门优化。这种分离式设计不仅消除了阶段间的干扰，还能显著提升系统的有效吞吐量（Goodput），为大规模 LLM 服务提供了更优的解决方案。</p><h2 id="_1-吞吐量-throughput-vs-有效吞吐量-goodput" tabindex="-1">1 吞吐量（Throughput）vs 有效吞吐量（Goodput） <a class="header-anchor" href="#_1-吞吐量-throughput-vs-有效吞吐量-goodput" aria-label="Permalink to &quot;1 吞吐量（Throughput）vs 有效吞吐量（Goodput）&quot;">​</a></h2><p>目前，大多数 LLM 服务系统（如 vLLM、TensorRT-LLM）都以吞吐量（Throughput） 作为主要性能指标——即单位时间内处理的请求数（RPS）或生成的 token 数。这种度量方式直观，并且与成本（$/req）有直接关联，因此被广泛采用。</p><p>实际上，下游应用的类型多种多样，它们在用户体验上的延迟需求各异，因此需要满足的服务等级目标（SLO）也存在显著差异。大模型服务中最常用的 SLO 包括：</p><ul><li>TTFT (Time To First Token)：首 token 响应延迟，直接影响用户的等待体验。</li><li>TPOT (Time Per Output Token)：衡量两个连续生成的 token 之间的平均延迟，决定交互的流畅程度。</li></ul><p>例如，实时聊天机器人更关注低 TTFT 以保证响应及时，而 TPOT 只需快于人类阅读速度（约 250 词/分钟）即可；相反，文档摘要则更强调低 TPOT，以便更快地产生完整摘要。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250918230251121.png" alt=""></p><p>单纯依赖 Throughput 作为指标，并不能反映延迟表现，系统看似处理了大量请求，但其中不少未能满足 SLO，最终呈现给用户的仍是不理想的服务体验。</p><ul><li><strong>Throughput（吞吐量）</strong>：通常指系统单位时间内处理的 token 数或请求数。很多工作把“提高吞吐量”作为主要优化目标，但在实际场景下，这并不直接代表用户体验。</li><li><strong>Goodput（有效吞吐量）</strong>：指系统在满足延迟约束（如 TTFT/TPOT SLO）的前提下，真正完成的请求数量。如果一个请求因为延迟过长而被用户放弃，或者超过服务约束而无效，那么即便它产生了 token，也不能算作有效产出。</li></ul><p>在 <a href="https://arxiv.org/abs/2401.09670" target="_blank" rel="noreferrer">DistServe</a> 论文中，引入了 Goodput 概念，<strong>即在满足 SLO（TTFT 和 TPOT 要求）的前提下，每秒完成的有效请求数</strong>。与单纯的吞吐量相比，Goodput 是更优的衡量指标，<strong>因为它能够体现请求在满足 SLO 情况下的吞吐水平</strong>，从而同时反映成本效益与服务质量。</p><p>为了简要说明 Goodput，假设某个应用要求至少 90% 的请求满足 TTFT &lt; 200ms 且 TPOT &lt; 50ms，则可以得到如下定义：</p><blockquote><p>Goodput (P90 TTFT &lt; 200ms 且 P90 TPOT &lt; 50ms) 表示在至少 90% 的请求同时满足 TTFT &lt; 200ms 和 TPOT &lt; 50ms 的条件下，系统所能维持的最大每秒请求数。</p></blockquote><p>下图展示了一个简单的例子：某应用的吞吐量为 10 RPS（每秒请求数），但由于延迟约束的限制，只有 3 RPS 的请求满足 SLO，因此该系统的 Goodput 仅为 3 RPS。可以想象，用户在这样一个 高吞吐但低 Goodput 的系统中，依然会感受到较差的服务体验。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250918232043135.png" alt=""></p><h2 id="_2-prefill-与-decode-共置导致干扰" tabindex="-1">2 Prefill 与 Decode 共置导致干扰 <a class="header-anchor" href="#_2-prefill-与-decode-共置导致干扰" aria-label="Permalink to &quot;2 Prefill 与 Decode 共置导致干扰&quot;">​</a></h2><p>在 LLM 服务中请求的生命周期通常包含两个阶段：prefill（生成首个 token）和 decode（逐步生成后续 token）。大多数现有系统（如 vLLM、TensorRT-LLM）采用 continuous batching 技术，将 prefill 和 decode 混合在一起统一批处理。这种方式确实能够提升整体吞吐量，但由于两者计算特性和 SLO 目标差异显著，将它们共置在同一 GPU 上往往并不理想。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919080109748.gif" alt=""></p><p>如下图所示，continuous batching 会带来明显的干扰。<strong>当 prefill 和 decode 被放在同一批次时，decode 请求的延迟（TPOT）会被显著拉长，而 prefill 请求的首 token 延迟（TTFT）也会有所增加。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919074140327.png" alt=""></p><p>图中展示了三种不同的执行方式：</p><ul><li>1P+nD（棕色柱子）：1 个 prefill 与 n 个 decode 混合批处理。</li><li>nD（蓝色柱子）：仅包含 decode 请求的批处理。</li><li>prefill-only（红色虚线）：仅运行 prefill 请求的延迟。</li></ul><p><strong>在 prompt 长度为 128 时，相比仅包含 decode 的请求，延迟增加约 1.8 倍；而当 prompt 长度为 1024 时，干扰效应显著放大，decode 延迟提升至 12.6 倍。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919074202351.png" alt=""></p><p>由于这种干扰，如下图所示，当服务必须同时满足 TTFT 和 TPOT 的 SLO 时，<strong>系统往往需要进行资源的过度配置才能达到延迟目标</strong>，尤其是在任一 SLO 要求较严格的情况下。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919075355980.png" alt=""></p><h2 id="_3-pd-分离的整体思路" tabindex="-1">3 PD 分离的整体思路 <a class="header-anchor" href="#_3-pd-分离的整体思路" aria-label="Permalink to &quot;3 PD 分离的整体思路&quot;">​</a></h2><p>直观的思路很简单：将 prefill 和 decode 分离到不同的 GPU 上，并为每个阶段定制并行策略。这自然解决了前面提到的两个问题：</p><ul><li><strong>没有干扰</strong>：prefill 和 decode 各自独立运行，更快地完成计算，也更容易满足各自的 SLO。</li><li><strong>资源分配与并行策略解耦</strong>：可以针对 prefill 和 decode 分别进行优化。</li></ul><p>下图展示了在这样一个分离式系统中，请求是如何被处理的。当一个请求到达系统时，它会先被分配到 prefill worker 完成 prefill 阶段；随后系统将其中间状态（主要是 KV Cache）迁移到 decode worker，并执行多步 decode 以生成后续 token；当生成完成后，请求才会离开系统。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919080133122.gif" alt=""></p><p>让我们通过一个简单的实验来看看为什么 PD 分离是有益的。我们在一张 A100-80GB GPU 上运行一个 130 亿参数的 LLM，请求到达服从泊松分布，输入长度为 512，输出长度为 64。我们逐步增加请求速率（x 轴），并在下图测量两类延迟（P90 TTFT 和 P90 TPOT，y 轴）的变化。</p><p>假设我们设定 SLO：P90 TTFT = 0.4 秒，P90 TPOT = 0.04 秒（下图中的横线）。实验结果表明：在单卡情况下，现有系统大约可以在 3 rps 下满足 TTFT 的延迟约束，而 TPOT 只能维持在 1.6 rps（下图左边）。由于必须同时满足两个约束条件，现有共置系统的 Goodput = min(3, 1.6) = 1.6 rps/GPU。</p><p>在分离之后，性能得到了显著提升。如果单独处理一个阶段，prefill worker 和 decode worker 的 rps 都优于之前的结果 —— 如下图右边所示，一个 prefill worker 大约可达到 5.6 rps，一个 decode worker 大约可达到 10 rps。更重要的是，我们现在可以灵活地分配资源，例如配置 2 个 prefill worker + 1 个 decode worker（记作 2P1D），共 3 张 GPU。此时：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Goodput</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> (2P1D) = min(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">5.6</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ×</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 2</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">,</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 10</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">) = 10 reqs/s ÷ 3 GPUs ≈ 3.3 rps/GPU。</span></span></code></pre></div><p>这个实验表明，即便没有引入任何并行优化，仅仅通过简单的分离，Goodput 就提升了约 2 倍。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250919080653076.png" alt=""></p><h2 id="_4-分离式推理架构的优化方向" tabindex="-1">4 分离式推理架构的优化方向 <a class="header-anchor" href="#_4-分离式推理架构的优化方向" aria-label="Permalink to &quot;4 分离式推理架构的优化方向&quot;">​</a></h2><h3 id="_4-1-算力与存储" tabindex="-1">4.1 算力与存储 <a class="header-anchor" href="#_4-1-算力与存储" aria-label="Permalink to &quot;4.1 算力与存储&quot;">​</a></h3><ul><li>prefill 阶段：拥有计算受限的性质（compute-bound），特别是在请求流量较大，用户的 prompt 也比较长的情况下。prefill 阶段算完 KV cache 并发给 decode 阶段后，理论上 prefill 就不再需要这个 KV cache 了（当然你也可以采用 LRU 等策略对 KV cache 的保存做管理，而不是一股脑地清除）。</li><li>decode 阶段：拥有内存受限的性质（memory-bound），因为逐个 token 的生成方式，decode 阶段要频繁从内存中读取 KV Cache，同时也意味着它需要尽可能保存 KV cache。</li></ul><p>因此在分离式框架下，计算和存储可以朝着两个独立的方向做优化。</p><h3 id="_4-2-batching-策略" tabindex="-1">4.2 Batching 策略 <a class="header-anchor" href="#_4-2-batching-策略" aria-label="Permalink to &quot;4.2 Batching 策略&quot;">​</a></h3><ul><li>prefill 阶段：随着 batch size 的增加，吞吐量的提升很快趋于平缓。这是因为 prefill 属于 compute-bound，当 batch 中的总 tokens 数超过一定规模后，GPU 的计算能力已经被完全吃满，再增加请求只会延长整体处理时间，而不会带来明显的吞吐提升。</li><li>decode 阶段：随着 batch size 的增加，吞吐量的增长趋势越来越显著。这是因为 decode 阶段是 memory-bound，即相比于计算，读写数据的时间要更多。所以在 decode 阶段中，如果我们能提升 batch size，就能把计算强度提起来，吞吐量就上去了。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250920102734489.png" alt=""></p><p>在分离架构下，我们可以针对 prefill 和 decode 的特性对 batching 策略分别进行优化：</p><ul><li>具体来说，对于 prefill 实例，需要事先结合特定的 LLM 和 GPU 做性能分析，找出输入长度的临界点——一旦超过这个点，prefill 就会进入 compute-bound，此时增加 batch size 只会拖慢整体处理速度。在实际应用中，用户的 prompt 往往已有数百个 tokens，因此 <strong>prefill 的 batch size 通常保持较小</strong>。</li><li><strong>相对地，decode 阶段更适合采用较大的 batch size，以充分提升 GPU 利用率和整体吞吐。</strong></li></ul><h3 id="_4-3-并行策略" tabindex="-1">4.3 并行策略 <a class="header-anchor" href="#_4-3-并行策略" aria-label="Permalink to &quot;4.3 并行策略&quot;">​</a></h3><p>由于 prefill 和 decode 具有不同的计算模式和延迟目标，这两个阶段的最佳并行策略通常并不相同。例如，当 TTFT 要求严格而 TPOT 要求相对宽松时，prefill 更适合采用<strong>张量并行</strong>来满足低延迟，而 decode 则通常采用<strong>数据并行</strong>或<strong>流水线</strong>并行来提升吞吐。</p><h2 id="_5-kv-cache-传输" tabindex="-1">5 KV Cache 传输 <a class="header-anchor" href="#_5-kv-cache-传输" aria-label="Permalink to &quot;5 KV Cache 传输&quot;">​</a></h2><p>PD 分离带来的代价是需要在 prefill 和 decode 的 GPU 之间传输中间状态（即 KV cache）。接下来，我们来看看 KV cache 传输的开销分析、传输方式以及相关的优化策略。</p><h3 id="_5-1-kv-cache-传输开销" tabindex="-1">5.1 KV Cache 传输开销 <a class="header-anchor" href="#_5-1-kv-cache-传输开销" aria-label="Permalink to &quot;5.1 KV Cache 传输开销&quot;">​</a></h3><p>初看之下，KV cache 是 LLM 推理中巨大的内存开销，而 GPU 之间 KV cache 的传输似乎会成为瓶颈。然而，DistServe 的论文中展示了相反的结果：通过合理的放置，KV Cache 的传输开销可以被有效地最小化，甚至低于一次 decode 步骤的时间，这得益于当今高速互联网络（如 NVLink 和 PCI-e 5.0）。</p><p>假设我们在 GPU 之间使用 8 通道 PCIe 5.0 x16（每条链路 64GB/s）作为节点内互联。对于一个包含 2048 tokens 的请求，在服务 OPT-175B 时传输 KV cache 的延迟可以估算如下：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Latency</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 2048</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> *</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> (4.5 </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">MB/token</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">) / (</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">64GB/s</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> *</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 8</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">) = 17.6 ms</span></span></code></pre></div><p>对于 OPT-175B，延迟小于单次 decode 步骤（在 A100 上约为 30-50 毫秒）。对于更大的模型、更长的序列或更先进的网络（例如带宽为 600GB/s 的 A100-NVLink），如下图所示，与单次 decoe 步骤相比，KV cache 传输相关的相对开销变得不那么显著。总之，通过精心安排 prefill 和 decode 工作节点以利用高带宽网络，可以有效隐藏 KV cache 传输的开销。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250920113307725.png" alt=""></p><h3 id="_5-2-kv-cache-传输方式" tabindex="-1">5.2 KV Cache 传输方式 <a class="header-anchor" href="#_5-2-kv-cache-传输方式" aria-label="Permalink to &quot;5.2 KV Cache 传输方式&quot;">​</a></h3><p>目前 KV cache 的传输主要有两种方式：<strong>中心存储</strong>和<strong>点对点（P2P）</strong>，当然在实际系统中也可能采用二者结合的混合方案。</p><ul><li><strong>中心存储</strong>：建立一个跨设备的 KV store，由它统一管理 KV cache 的增、删、查和传递等操作。prefill 和 decode 实例只需与这个 KV store 交互，负责写入或读取数据。</li><li><strong>P2P 传输</strong>：每个实例独立管理自己的存储。例如，一个 prefill 实例完成计算后，会直接与目标 decode 实例建立通信，将 KV cache 传过去，不依赖统一的中介。</li></ul><p>两种方式各有优劣：</p><ul><li><strong>中心存储</strong>：更适合构建大规模集群，能充分利用多种存储介质和传输通道，并提升计算结果的复用效率，但在某些场景下性能可能受限，同时系统维护成本较高。</li><li><strong>P2P 传输</strong>：架构更简单，性能表现通常更好，但在扩展性和链路稳定性方面会面临挑战。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250920224320677.png" alt=""></p><h3 id="_5-3-kv-cache-传输的网络堆栈" tabindex="-1">5.3 KV Cache 传输的网络堆栈 <a class="header-anchor" href="#_5-3-kv-cache-传输的网络堆栈" aria-label="Permalink to &quot;5.3 KV Cache 传输的网络堆栈&quot;">​</a></h3><p>现有的物理数据链路可以分为 3 类：</p><ul><li><strong>Direct</strong>，即 GPU 之间通过高速直连链路（如 NVLink 或 HCCS）相互连接。在这种情况下，可以利用底层的内存拷贝原语或集体通信库来完成数据传输。</li><li><strong>Direct-NIC</strong>，即 GPU 通过其配套的网卡（NIC）进行通信。在这里，可以使用定制化的库，通过 PCIe 和以太网（或 InfiniBand）进行数据传输。</li><li><strong>Indirect</strong>，即当 GPU 之间没有直接链路时，必须通过其 CPU 的 DRAM 中转数据，从而带来额外的内存拷贝开销。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250920225127533.png" alt=""></p><p><a href="https://arxiv.org/abs/2401.11181" target="_blank" rel="noreferrer">图片来源：Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads</a></p><h3 id="_5-4-kv-cache-传输粒度" tabindex="-1">5.4 KV Cache 传输粒度 <a class="header-anchor" href="#_5-4-kv-cache-传输粒度" aria-label="Permalink to &quot;5.4 KV Cache 传输粒度&quot;">​</a></h3><p>KV cache 传输粒度可以分为 3 类：</p><ul><li><strong>请求级</strong>：等到 prefill 阶段完成后，将 KV cache 一次性传输。这种方式的好处是能够减少网络传输次数，因为每次传输的数据量更大，从而降低了通信开销。然而当 KV cache 大小较大时，会影响 TTFT 的延迟。</li><li><strong>层级</strong>：Splitwise 通过在 prefill 阶段的计算与 KV cache 传输之间实现重叠来优化性能。**每一层计算完成后，都会异步传输该层的 KV cache，同时继续执行下一层的计算，从而降低传输开销。**层级传输还能带来额外优势，例如更早启动 decode 阶段，以及更早释放 prefill 端的内存。层级 KV cache 传输与下一层的 prefill 计算并行进行，这需要逐层的细粒度同步以确保正确性，因此可能会带来性能干扰并增加 TTFT，尤其是在小 prompt 的场景下。不过对于小 prompt 来说，KV cache 的总体规模很小，不需要层级传输来隐藏延迟。由于在计算开始时批次中的 token 数已经是已知的，<strong>Splitwise 会选择最合适的 KV cache 传输方式：小 prompt 使用序列化传输，而大 prompt 使用层级传输。</strong></li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250920232717989.png" alt=""></p><p><a href="https://arxiv.org/abs/2311.18677" target="_blank" rel="noreferrer">图片来源：Splitwise: Efficient Generative LLM Inference Using Phase Splitting</a></p><ul><li><strong>块级</strong>：TetriInfer 在 PD 分离的基础上，还会将输入的 prompt 划分为固定大小的 chunk，以便让 GPU 始终运行在接近计算饱和的状态。因此，TetriInfer 论文中也提出了基于块级的 KV cache 传输方案。</li></ul><h2 id="_6-vllm-的-pd-分离" tabindex="-1">6 vLLM 的 PD 分离 <a class="header-anchor" href="#_6-vllm-的-pd-分离" aria-label="Permalink to &quot;6 vLLM 的 PD 分离&quot;">​</a></h2><p>vLLM 提供了 <strong>KV Connector</strong> 作为管理实例间 KV cache 交换的抽象层，它提供统一接口来实现 KV cache 的保存、加载与传输，使不同的 vLLM 实例（如 prefill 与 decode 实例）能够高效共享计算结果。通过实现这一接口，各类 connector（例如通过文件系统的 SharedStorageConnector、通过网络的 NixlConnector 等）提供了灵活的 KV cache 传输方案，从而支持 PD 分离等高级功能。</p><p><code>KVConnectorBase_V1</code> 是所有 connector 的基类。它是一个抽象基类，定义了以下 API：</p><ul><li><p>scheduler 侧方法：</p><ul><li>build_connector_meta：构建元数据，scheduler 告诉 worker 需要保持/加载哪些 KV cache。</li><li>get_num_new_matched_tokens：获取远端已计算的 KV cache 的 token 数量。</li><li>update_state_after_alloc：block 开辟后，更新 connector 的状态。</li></ul></li><li><p>worker 侧方法：</p><ul><li><strong>start_load_kv</strong>：从 connector buffer 加载 KV cache，消费端调用。</li><li>wait_for_layer_load：阻塞直到指定层加载结束，消费端调用；</li><li><strong>save_kv_layer</strong>：将 vLLM 的 KV buffer 中某一层的 KV cache 保存到 connector buffer 中，生产端调用。</li><li>wait_for_save：阻塞直到所有保存操作完成，生产端调用。</li></ul></li></ul><p>vLLM v1 中 connector 有两个执行角色（Role）：scheduler_connector 和 worker_connector，分别在 scheduler 线程和 worker 线程中执行。<strong>scheduler 负责指挥 worker 进行 KV cache 的传递，两者之间的信息桥梁是元数据（KVConnectorMetadata），worker 通过 metadata 知道哪些 KV 值需要从远端加载。</strong></p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250921115717450.png" alt=""></p><p>当前 vLLM 支持 5 种类型的 connector，分别是：</p><ul><li><strong>SharedStorageConnector</strong>：SharedStorageConnector 是 vLLM 中最简单的 KV Connector 实现，它通过共享文件系统（如本地磁盘或 NFS）在 prefill 和 decode 实例之间传递 KV cache，使用 MD5 哈希生成唯一文件名来存储和检索每个请求的 KV cache。prefill 实例将每层的 KV cache 序列化为 SafeTensors 格式保存到指定路径，decode 实例根据相同的 token_ids 计算哈希值找到对应文件并加载，整个过程没有显式的网络传输，完全依赖文件系统的读写操作。</li><li><strong>P2pNcclConnector</strong>：P2pNcclConnector 是基于 NCCL（NVIDIA Collective Communications Library）实现的高性能 KV Connector，它通过 NCCL 的 send/recv 原语实现 KV cache 在不同 GPU 之间的点对点传输，避免了文件系统的开销。</li><li><strong>NixlConnector</strong>：NixlConnector 使用 NIXL（NVIDIA Inference Xfer Library）库来加速 GPU 之间以及异构内存与存储之间的 KV cache 传输。</li><li><strong>LMCacheConnectorV1</strong>：通过与 LMCache 集成实现 KV cache 的外部存储和检索，支持多种<a href="https://docs.lmcache.ai/getting_started/quickstart/offload_kv_cache.html#supported-offloading-destinations" target="_blank" rel="noreferrer">存储后端</a>（如 CPU 内存、本地文件系统、Redis、 InfiniStore 等）。LMCache 通过重用缓存的 KV cache 来减少推理时间，消除冗余计算，适用于跨请求或跨会话的 KV cache 共享场景。</li><li><strong>MultiConnector</strong>：允许同时使用多个 KV connector 来实现 KV cache 的传输，它的核心逻辑是从第一个能提供可用 token 的 connector 加载 KV cache，但会向所有 connector 保存数据。MultiConnector 适用于需要同时向多个存储后端保存 KV cache 的场景，比如同时保存到本地存储和远程存储，提供数据冗余和可靠性保障。</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">--kv-transfer-config</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   &quot;kv_connector&quot;: &quot;MultiConnector&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   &quot;kv_connector_extra_config&quot;: {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">      &quot;connectors&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">         {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;kv_connector&quot;: &quot;NixlConnector&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;kv_role&quot;: &quot;kv_both&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">         },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">         {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;kv_connector&quot;: &quot;SharedStorageConnector&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;kv_connector_extra_config&quot;: {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">               &quot;shared_storage_path&quot;: &quot;local_storage&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;kv_role&quot;: &quot;kv_both&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">         }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">      ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   &quot;kv_role&quot;: &quot;kv_both&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><p>以上几个 connector 的运行实例代码可以在这里找到：<a href="https://docs.vllm.ai/en/latest/features/disagg_prefill.html#usage-example" target="_blank" rel="noreferrer">https://docs.vllm.ai/en/latest/features/disagg_prefill.html#usage-example</a></p><h2 id="_7-pd-分离工业界项目" tabindex="-1">7 PD 分离工业界项目 <a class="header-anchor" href="#_7-pd-分离工业界项目" aria-label="Permalink to &quot;7 PD 分离工业界项目&quot;">​</a></h2><h3 id="_7-1-mooncake" tabindex="-1">7.1 Mooncake <a class="header-anchor" href="#_7-1-mooncake" aria-label="Permalink to &quot;7.1 Mooncake&quot;">​</a></h3><p><a href="https://kvcache-ai.github.io/Mooncake/index.html" target="_blank" rel="noreferrer">Mooncake</a> 是 Moonshot AI 提供的领先大模型服务 Kimi 的推理平台。Mooncake 是 PD 分离应用比较早也是规模比较大的成功例子：</p><ul><li>Mooncake 采用了以 KV cache 为核心的解耦架构，将 prefill 集群与 decode 集群分离。</li><li>同时，它还利用 GPU 集群中未被充分利用的 CPU、DRAM 和 SSD 资源，实现了解耦式的 KV cache 缓存。</li><li>Mooncake 的核心是一个以 KV cache 为中心的调度器，它在最大化整体有效吞吐量和满足延迟相关的 SLO 之间进行平衡。</li><li>与传统假设所有请求都会被处理的研究不同，Mooncake 需要应对高负载场景带来的挑战。为此，Mooncake 提出了一种基于预测的早期拒绝策略。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250921174816597.png" alt=""></p><p><a href="https://arxiv.org/abs/2407.00079" target="_blank" rel="noreferrer">图片来源：Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></p><p>实验结果表明，Mooncake 在 长上下文场景下表现尤为突出：在某些模拟场景中，吞吐量相比基线方法最高可提升 525%，同时仍能满足 SLO。在真实负载下，Mooncake 的创新架构使 Kimi 能够多处理 75% 的请求。</p><p>使用 Mooncake 运行 PD 分离服务请参考文档：<a href="https://kvcache-ai.github.io/Mooncake/getting_started/examples/vllm-integration/vllmv1-lmcache-integration.html" target="_blank" rel="noreferrer">vLLM V1 Disaggregated Serving with Mooncake Store and LMCache</a></p><h3 id="_7-2-dynamo" tabindex="-1">7.2 Dynamo <a class="header-anchor" href="#_7-2-dynamo" aria-label="Permalink to &quot;7.2 Dynamo&quot;">​</a></h3><p><a href="https://docs.nvidia.com/dynamo/latest/index.html" target="_blank" rel="noreferrer">NVIDIA Dynamo</a> 是一个开源的模块化推理框架，用于在分布式环境上实现生成式 AI 模型的服务化部署。Dynamo 通过动态资源调度、智能路由、内存优化与高速数据传输，无缝扩展大型 GPU 集群之间的推理工作负载。</p><p>Dynamo 采用推理引擎无关的设计（支持 TensorRT-LLM、vLLM、SGLang 等），包括以下 4 个核心组件：</p><ul><li><strong>NVIDIA Dynamo Planner</strong>：一个智能规划和调度引擎，用于监控分布式推理中的容量与延迟，并在 prefill 与 decode 阶段之间灵活分配 GPU 资源，以最大化吞吐量和效率。Planner 会持续跟踪关键的 GPU 容量指标，并结合应用的 SLO（如 TTFT 和 ITL），智能决策是否采用分离式推理，或是否需要为 prefill/decode 阶段动态增加更多 GPU。</li><li><strong>NVIDIA Dynamo Smart Router</strong>：KV cache 感知的路由引擎，可在分布式推理环境中将请求转发到最佳的节点，从而最大限度减少 KV cache 的重复计算开销。</li><li><strong>NVIDIA Dynamo Distributed KV Cache Manager</strong>：通过将较旧或低频访问的 KV cache 卸载到更低成本的存储（如 CPU 内存、本地存储或对象存储等），大幅降低 GPU 内存占用。借助这种分层管理，开发者既能保留大规模 KV cache 重用的优势，又能释放宝贵的 GPU 资源，从而有效降低推理计算成本。</li><li><strong>NVIDIA Inference Transfer Library (NIXL)</strong>：高效的推理数据传输库，可加速 GPU 之间以及异构内存与存储之间的 KV cache 传输。通过减少同步开销和智能批处理，NIXL 显著降低了分布式推理中的通信延迟，使得在 prefill/decode 分离部署时，prefill 节点也能在毫秒级将大批量的 KV cache 传输至 decode 节点，从而避免跨节点数据交换成为性能瓶颈。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250905101610730.png" alt=""></p><p>在 Dynamo 的 PD 分离架构中，有 4 个核心组件：</p><ul><li><strong>(decode) worker</strong>：执行 prefill 和 decode 请求。</li><li><strong>prefill worker</strong>：只执行 prefill 请求。</li><li><strong>disaggregated router</strong>：决定 prefill 阶段是在本地还是远程执行。</li><li><strong>prefill queue</strong>：缓存并负载均衡远程 prefill 请求。</li></ul><p>当 worker 收到请求时，首先会通过 disaggregated router 判断 prefill 应该在本地还是远程完成，并分配相应的 KV block。 如果选择远程 prefill，请求会被推送到 prefill queue。随后，prefill worker 从队列中取出请求，读取 worker 中 prefix cache 命中的 KV block，执行 prefill 计算，并将生成的 KV block 回写给 worker。最后，worker 会继续完成剩余的 decode 阶段。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250905210843098.png" alt=""></p><p>Dynamo 提供了 Operator 方便我们在 Kubernetes 环境中以声明式的方式定义 PD 分离服务。只需在 <code>DynamoGraphDeployment</code> 配置中声明 Frontend、VllmDecodeWorker 和 VllmPrefillWorker 三个组件即可。<code>dynamoNamespace</code> 是 Dynamo 分布式运行时的逻辑隔离单元，而非 Kubernetes 的 namespace；同一 <code>dynamoNamespace</code> 内的组件可以相互发现并进行通信。</p><div class="language-yaml vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">yaml</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code"><code><span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">apiVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">nvidia.com/v1alpha1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">kind</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">DynamoGraphDeployment</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">metadata</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">  name</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">vllm-disagg</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">spec</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">  services</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">    Frontend</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      dynamoNamespace</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">vllm-disagg</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      componentType</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">frontend</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      replicas</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      extraPodSpec</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">        mainContainer</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          image</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.4.1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">    VllmDecodeWorker</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      dynamoNamespace</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">vllm-disagg</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      componentType</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">worker</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      replicas</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      resources</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">        limits</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          gpu</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;1&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      extraPodSpec</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">        mainContainer</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          image</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.4.1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          workingDir</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/workspace/components/backends/vllm</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          command</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/bin/sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">-c</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          args</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;python3 -m dynamo.vllm --model Qwen/Qwen3-0.6B&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">    VllmPrefillWorker</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      dynamoNamespace</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">vllm-disagg</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      componentType</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">worker</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      replicas</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      resources</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">        limits</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          gpu</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;1&quot;</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">      extraPodSpec</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">        mainContainer</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          image</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.4.1</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          workingDir</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/workspace/components/backends/vllm</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          command</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/bin/sh</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">-c</span></span>
<span class="line"><span style="--shiki-light:#22863A;--shiki-dark:#8DDB8C;">          args</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">            - </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;python3 -m dynamo.vllm --model Qwen/Qwen3-0.6B --is-prefill-worker&quot;</span></span></code></pre></div><p>Dynamo 详细的部署教程可以参考博客：<a href="https://cr7258.github.io/blogs/original/2025/20-dynamo#_3-%E8%BF%90%E8%A1%8C-dynamo" target="_blank" rel="noreferrer">https://cr7258.github.io/blogs/original/2025/20-dynamo#_3-运行-dynamo</a></p><h3 id="_7-3-llm-d" tabindex="-1">7.3 llm-d <a class="header-anchor" href="#_7-3-llm-d" aria-label="Permalink to &quot;7.3 llm-d&quot;">​</a></h3><p><a href="https://llm-d.ai/docs/architecture" target="_blank" rel="noreferrer">llm-d</a> 是一个 Kubernetes 原生的分布式推理服务栈，为团队提供一条清晰高效的路径，以最快的落地速度在大规模环境中部署并管理推理服务。llm-d 通过集成业界标准的开源技术来加速分布式推理：使用 vLLM 作为模型服务与引擎，Inference Gateway 作为请求调度器与负载均衡器，Kubernetes 作为基础设施编排与工作负载控制平面。</p><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250921175300426.png" alt=""></p><p>llm-d 提供以下核心功能：</p><ul><li>基于 vLLM 优化的推理调度器：llm-d 基于 IGW 的 Endpoint Picker Protocol (EPP) 实现可定制化的“智能”负载均衡，专门针对 vLLM 进行优化。调度器结合运行时遥测数据，利用过滤和打分算法，在 P/D 分离、KV cache、SLA 与负载感知的基础上做出调度决策，同时支持团队自定义策略。</li><li>PD 分离：llm-d 利用 vLLM 的分离式推理能力，将 prefill 和 decode 拆分到独立实例运行，并通过高性能传输库（如 NIXL）进行通信。</li><li>分布式 KV cache：llm-d 使用 vLLM 的 KV connector 构建可插拔的 KV cache 层级体系，支持将 KV cache 卸载到主机、远程存储或 LMCache 等系统。</li></ul><p>使用 llm-d 运行 PD 分离服务请参考：<a href="https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation" target="_blank" rel="noreferrer">https://github.com/llm-d/llm-d/tree/main/guides/pd-disaggregation</a></p><h3 id="_7-4-aibrix" tabindex="-1">7.4 AIBrix <a class="header-anchor" href="#_7-4-aibrix" aria-label="Permalink to &quot;7.4 AIBrix&quot;">​</a></h3><p>AIBrix 是字节跳动开源的云原生分布式推理框架，专为大规模 LLM 部署设计。</p><ul><li>AIBrix 支持 LoRA 管理、前缀感知和负载感知的智能路由，并通过分布式 KV cache 实现跨节点的高效 token 复用。</li><li>在系统编排层面，AIBrix 结合 Kubernetes 与 Ray 的混合调度，既能满足大规模集群管理的需求，又能灵活执行细粒度任务。</li><li>AIBrix 基于 SLO 的 GPU 优化器与诊断工具进一步提升了资源利用率和系统可靠性。</li></ul><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/20250921181737485.png" alt=""></p><p>使用 AIBrix 运行 PD 分离服务请参考：<a href="https://github.com/vllm-project/aibrix/tree/main/samples/disaggregation/vllm" target="_blank" rel="noreferrer">https://github.com/vllm-project/aibrix/tree/main/samples/disaggregation/vllm</a></p><h2 id="_8-chunked-prefills-vs-pd-分离" tabindex="-1">8 Chunked-Prefills VS PD 分离 <a class="header-anchor" href="#_8-chunked-prefills-vs-pd-分离" aria-label="Permalink to &quot;8 Chunked-Prefills VS PD 分离&quot;">​</a></h2><p>chunked-prefills 方案的核心思想是：将长序列的 prefill 请求拆分为几乎相等大小的小块，然后构建了由 prefill 小块和 decode 组成的混合 batch。或者说，chunked-prefills 策略通过将不同长度的 prompts 拆分成长度一致的 chunks 来进行 prefill，以避免长 prompt 阻塞其他请求，同时利用这些 chunks 的间隙进行 decode 的插入/捎带（piggyback）操作，从而减少延迟并提高整体的吞吐。</p><p>decode 阶段的开销不仅来自从 GPU 内存中获取 KV cache，还包括提取模型参数。而通过这种 piggyback 方法，decode 阶段能够重用 prefill 时已提取的模型参数，几乎将 decode 阶段从一个以内存为主的操作转变为一个计算为主的操作。因此，这样构建的混合批次具有近乎均匀的计算需求（而且增加了计算密集性），使我们能够创建平衡的微批处理调度，缓解了迭代之间的不平衡，导致 GPU 的管道气泡最小化，提高了 GPU 的利用率。也最小化了计算新 prefill 对正在进行的 decode 的 TBT 的影响，从而实现了高吞吐量和低 TBT 延迟。</p><p>chunked-prefills 有两个明显的好处：</p><ul><li>所有节点被平等对待，使调度更简单。</li><li>将 chunked prefill 内联到 decode 批处理中可以提高 decode 批次的计算强度，从而带来更好的 MFU（Model FLOPs Utilization，指的是模型实际使用的计算量占 GPU 理论峰值算力的比例，用来衡量算力利用效率）。</li></ul><p>然而，chunked-prefills 也有一些缺点：</p><ul><li>chunked-prefills 会增加 prefill 的计算开销，如果 chunk 大小明显低于 GPU 饱和点，会延长 prefill 的执行时间。</li><li>prefill 阶段仍难以完全最大化 MFU，因为在 chunk-prefill 中，profiling 只会估算特定设备上一个 batch 的最大 tokens 配额，这个配额同时包含 prefill 和 decode，而不是分别针对两者优化。</li><li>chunked-prefills 也会显著增加 prefill 阶段的内存访问量，每个 chunk 的 Attention 操作都需重复读取此前的 KV cache，增加内存访问负担。而且长序列可能会持久地占据着 KV cache 的存储空间以及 GPU 的计算资源。</li><li>在 TPOT 方面，将 prefill 与 decode 合并批处理实际上会降低所有 decode 任务的平均速度。</li></ul><p><strong>总之，chunked-prefills 可能有助于最大化整体吞吐量，但由于动态分割无法完全解耦 prefill 和 decode 操作，会导致资源争用以及 TTFT 与 TPOT 之间的妥协。当应用程序无法在 TTFT 和 TPOT 之间进行权衡，而是要同时遵守这两者时，PD 分离就成为更好的选择。</strong></p><h2 id="_9-pd-分离相关论文" tabindex="-1">9 PD 分离相关论文 <a class="header-anchor" href="#_9-pd-分离相关论文" aria-label="Permalink to &quot;9 PD 分离相关论文&quot;">​</a></h2><ul><li><a href="https://arxiv.org/abs/2401.09670" target="_blank" rel="noreferrer">DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving</a></li><li><a href="https://arxiv.org/abs/2311.18677" target="_blank" rel="noreferrer">Splitwise: Efficient Generative LLM Inference Using Phase Splitting</a></li><li><a href="https://arxiv.org/abs/2401.11181" target="_blank" rel="noreferrer">TetriInfer: Inference without Interference:Disaggregate LLM Inference for Mixed Downstream Workloads</a></li><li><a href="https://arxiv.org/abs/2406.17565" target="_blank" rel="noreferrer">MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool</a></li><li><a href="https://arxiv.org/abs/2407.00079" target="_blank" rel="noreferrer">Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving</a></li></ul><h2 id="_10-总结" tabindex="-1">10 总结 <a class="header-anchor" href="#_10-总结" aria-label="Permalink to &quot;10 总结&quot;">​</a></h2><p>PD 分离大模型推理中的一种架构优化策略，核心思想是把 prefill 阶段和 decode 阶段分开，由不同的 GPU 或实例分别承担。通过分离架构，系统可以针对 prefill（计算密集型）和 decode（内存密集型）的不同特性分别优化资源配置和并行策略，从而在满足 TTFT 和 TPOT SLO 约束的前提下显著提升有效吞吐量（Goodput）。虽然 PD 分离需要在 GPU 间传输 KV Cache，但通过高速互联网络和优化的传输策略，这一开销可以被有效隐藏。目前，vLLM、Mooncake、Dynamo 等主流推理框架都已支持 PD 分离，为大规模 LLM 服务提供了更高效的解决方案。相比于 chunked-prefills 等替代方案，PD 分离在需要同时满足严格 TTFT 和 TPOT 要求的场景下具有明显优势。</p><h2 id="_11-参考资料" tabindex="-1">11 参考资料 <a class="header-anchor" href="#_11-参考资料" aria-label="Permalink to &quot;11 参考资料&quot;">​</a></h2><ul><li>Lecture 58: Disaggregated LLM Inference：<a href="https://www.youtube.com/watch?v=tIPDwUepXcA" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=tIPDwUepXcA</a></li><li>Throughput is Not All You Need: Maximizing Goodput in LLM Serving using Prefill-Decode Disaggregation：<a href="https://hao-ai-lab.github.io/blogs/distserve/" target="_blank" rel="noreferrer">https://hao-ai-lab.github.io/blogs/distserve/</a></li><li>Mooncake阅读笔记：深入学习以Cache为中心的调度思想，谱写LLM服务降本增效新篇章：<a href="https://zhuanlan.zhihu.com/p/706097807" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/706097807</a></li><li>探秘Transformer系列之（26）--- KV Cache优化 之 PD分离or合并：<a href="https://www.cnblogs.com/rossiXYZ/p/18815541" target="_blank" rel="noreferrer">https://www.cnblogs.com/rossiXYZ/p/18815541</a></li><li>大模型推理分离架构五虎上将：<a href="https://zhuanlan.zhihu.com/p/706218732" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/706218732</a></li><li>LLM关于PD分离的最新实测：<a href="https://zhuanlan.zhihu.com/p/1919794916504114120" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1919794916504114120</a></li><li>State of the Model Serving Communities - August 2025：<a href="https://inferenceops.substack.com/p/state-of-the-model-serving-communities" target="_blank" rel="noreferrer">https://inferenceops.substack.com/p/state-of-the-model-serving-communities</a></li><li>图解大模型训练系列：序列并行1，Megatron SP：<a href="https://zhuanlan.zhihu.com/p/4083427292" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/4083427292</a></li><li>序列并行做大模型训练，你需要知道的六件事：<a href="https://zhuanlan.zhihu.com/p/698031151" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/698031151</a></li><li>vLLM PD分离KV cache传递机制详解与演进分析：<a href="https://zhuanlan.zhihu.com/p/1906741007606878764" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1906741007606878764</a></li><li>vLLM PD分离方案浅析：<a href="https://zhuanlan.zhihu.com/p/1889243870430201414" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1889243870430201414</a></li><li>P/D Disaggregation of vLLM and Integration with Mooncake：<a href="https://docs.google.com/document/d/1Ab6TMW1E2CdHJJyCrpJnLhgmE2b_6leH5MVP9k72sjw/edit?tab=t.0#heading=h.611v2r4aqubz" target="_blank" rel="noreferrer">https://docs.google.com/document/d/1Ab6TMW1E2CdHJJyCrpJnLhgmE2b_6leH5MVP9k72sjw/edit?tab=t.0#heading=h.611v2r4aqubz</a></li><li>0.5x提升:PD分离KV cache传输的实践经验：<a href="https://zhuanlan.zhihu.com/p/1946608360259577576" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1946608360259577576</a></li><li>分布式推理优化思路：<a href="http://zhuanlan.zhihu.com/p/1937556222371946860" target="_blank" rel="noreferrer">http://zhuanlan.zhihu.com/p/1937556222371946860</a></li><li>图解大模型计算加速系列：分离式推理架构2，模糊分离与合并边界的chunked-prefills：<a href="https://zhuanlan.zhihu.com/p/710165390" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/710165390</a></li><li>图解大模型计算加速系列：分离式推理架构1，从DistServe谈起：<a href="https://zhuanlan.zhihu.com/p/706761664" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/706761664</a></li><li>LLM推理优化 - Prefill-Decode分离式推理架构：<a href="https://zhuanlan.zhihu.com/p/9433793184" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/9433793184</a></li><li>Shaping NIXL-based PD Disaggregation in vLLM V1：<a href="https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/" target="_blank" rel="noreferrer">https://blog.lmcache.ai/2025-04-11-lmcache-vllmv1-nixl/</a></li><li>vLLM P2P NCCL Connector：<a href="https://docs.vllm.ai/en/latest/design/p2p_nccl_connector.html" target="_blank" rel="noreferrer">https://docs.vllm.ai/en/latest/design/p2p_nccl_connector.html</a></li><li>vLLM Disaggregated Prefilling (experimental)：<a href="https://docs.vllm.ai/en/latest/features/disagg_prefill.html" target="_blank" rel="noreferrer">https://docs.vllm.ai/en/latest/features/disagg_prefill.html</a></li><li>LMCache Example: Disaggregated prefill：<a href="https://docs.lmcache.ai/getting_started/quickstart/disaggregated_prefill.html" target="_blank" rel="noreferrer">https://docs.lmcache.ai/getting_started/quickstart/disaggregated_prefill.html</a></li><li>Bringing State-Of-The-Art PD Speed to vLLM v1 with LMCache：<a href="https://blog.lmcache.ai/2025-04-29-pdbench/" target="_blank" rel="noreferrer">https://blog.lmcache.ai/2025-04-29-pdbench/</a></li><li>Demystify vLLM V1 KVconnector SharedStorageConnector：<a href="https://blog.diabloneo.com/demystify-vllm-v1-kvconnector-sharedstorageconnector-05a487627036" target="_blank" rel="noreferrer">https://blog.diabloneo.com/demystify-vllm-v1-kvconnector-sharedstorageconnector-05a487627036</a></li><li>vLLM源码之分离式架构：<a href="https://zhuanlan.zhihu.com/p/1933647687" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1933647687</a></li><li>vLLM v1 PD分离设计：<a href="https://zhuanlan.zhihu.com/p/1894425784107632241" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/1894425784107632241</a></li><li>Inside vLLM: Anatomy of a High-Throughput LLM Inference System：<a href="https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html" target="_blank" rel="noreferrer">https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html</a></li><li>[P/D][V1] KV Connector API V1：<a href="https://github.com/vllm-project/vllm/pull/15960" target="_blank" rel="noreferrer">https://github.com/vllm-project/vllm/pull/15960</a></li><li>vLLM PD Disaggregation discussion：<a href="https://docs.google.com/document/d/1uPGdbEXksKXeN4Q9nUm9hzotqEjQhYmnpAhidLuAsjk/edit?tab=t.0#heading=h.qhtgj3vmvwn" target="_blank" rel="noreferrer">https://docs.google.com/document/d/1uPGdbEXksKXeN4Q9nUm9hzotqEjQhYmnpAhidLuAsjk/edit?tab=t.0#heading=h.qhtgj3vmvwn</a></li><li>llm-d: Kubernetes-native Distributed Inference at Scale：<a href="https://github.com/llm-d/llm-d/blob/dev/docs/proposals/llm-d.md" target="_blank" rel="noreferrer">https://github.com/llm-d/llm-d/blob/dev/docs/proposals/llm-d.md</a></li></ul><h2 id="欢迎关注" tabindex="-1">欢迎关注 <a class="header-anchor" href="#欢迎关注" aria-label="Permalink to &quot;欢迎关注&quot;">​</a></h2><p><img src="https://chengzw258.oss-cn-beijing.aliyuncs.com/Article/202503222156941.png" alt=""></p></div></div></main><footer class="VPDocFooter" data-v-47739f1d data-v-351db90d><!--[--><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--><!--]--><div class="edit-info" data-v-351db90d><div class="edit-link" data-v-351db90d><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/cr7258/cr7258.github.io/edit/main/docs/courses/ai-infra/AI Infra 教程/06-disaggregating-prefill-and-decoding.md" target="_blank" rel="noreferrer" data-v-351db90d><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="edit-link-icon" aria-label="edit icon" data-v-351db90d><path d="M18,23H4c-1.7,0-3-1.3-3-3V6c0-1.7,1.3-3,3-3h7c0.6,0,1,0.4,1,1s-0.4,1-1,1H4C3.4,5,3,5.4,3,6v14c0,0.6,0.4,1,1,1h14c0.6,0,1-0.4,1-1v-7c0-0.6,0.4-1,1-1s1,0.4,1,1v7C21,21.7,19.7,23,18,23z"></path><path d="M8,17c-0.3,0-0.5-0.1-0.7-0.3C7,16.5,6.9,16.1,7,15.8l1-4c0-0.2,0.1-0.3,0.3-0.5l9.5-9.5c1.2-1.2,3.2-1.2,4.4,0c1.2,1.2,1.2,3.2,0,4.4l-9.5,9.5c-0.1,0.1-0.3,0.2-0.5,0.3l-4,1C8.2,17,8.1,17,8,17zM9.9,12.5l-0.5,2.1l2.1-0.5l9.3-9.3c0.4-0.4,0.4-1.1,0-1.6c-0.4-0.4-1.2-0.4-1.6,0l0,0L9.9,12.5z M18.5,2.5L18.5,2.5L18.5,2.5z"></path></svg> 不妥之处，敬请雅正<!--]--></a></div><div class="last-updated" data-v-351db90d><p class="VPLastUpdated" data-v-351db90d data-v-4c0877ea>最后更新: <time datetime="2025-09-21T14:13:15.000Z" data-v-4c0877ea></time></p></div></div><nav class="prev-next" data-v-351db90d><div class="pager" data-v-351db90d><a class="VPLink link pager-link prev" href="/courses/ai-infra/AI%20Infra%20%E6%95%99%E7%A8%8B/05-chunked-prefills" data-v-351db90d><!--[--><span class="desc" data-v-351db90d>上一篇</span><span class="title" data-v-351db90d>5. Chunked Prefills 分块预填充详解</span><!--]--></a></div><div class="pager" data-v-351db90d><!----></div></nav></footer><!--[--><!--[--><!--[--><div id="comment-container"></div><!--]--><!--]--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_index.md\":\"BK07kHrx\",\"about_me.md\":\"C0BSAwdi\",\"archives.md\":\"3jmyn70w\",\"blogs_original_2022_01-kubernetes-setup.md\":\"Dp8afgSw\",\"blogs_original_2022_02-kubectl-debug.md\":\"BxoznmS-\",\"blogs_original_2022_03-finalizer.md\":\"CG5fgxf_\",\"blogs_original_2022_04-kubeasz.md\":\"CUaEMuuc\",\"blogs_original_2022_05-docker-rootless.md\":\"AJPyXuNU\",\"blogs_original_2022_06-kafka-setup.md\":\"hyAQlpPm\",\"blogs_original_2022_07-kafka-big-message.md\":\"xJtlq2FI\",\"blogs_original_2022_08-reset-kafka-consumer-offset.md\":\"18EmGpPJ\",\"blogs_original_2022_09-pulsar.md\":\"0LpQWg_X\",\"blogs_original_2022_10-harbor.md\":\"x-Jz_o4j\",\"blogs_original_2022_11.nebula.md\":\"B_tE7fss\",\"blogs_original_2022_12.lru.md\":\"Csqh5zeA\",\"blogs_original_2022_13-envoy-quickstart.md\":\"Cue-0XEN\",\"blogs_original_2023_01-argocd-quickstart.md\":\"BE7mNaiY\",\"blogs_original_2023_02-wasm-in-cloud-native.md\":\"CoSlnYrK\",\"blogs_original_2023_03-kubectl-patch.md\":\"q0Sr89jz\",\"blogs_original_2023_04-kubernetes-keycloak-oidc.md\":\"C9jKv05O\",\"blogs_original_2023_05-kubernetes-multi-cluster-1.md\":\"BXytvG2k\",\"blogs_original_2023_06-kubernetes-multi-cluster-2.md\":\"B4XX7wzF\",\"blogs_original_2023_07-vcluster.md\":\"DI5Bt2Vl\",\"blogs_original_2023_08-cilium-cluster-mesh.md\":\"F23siFTt\",\"blogs_original_2023_09-clusterresourceset.md\":\"BKgjMVeW\",\"blogs_original_2023_10-cilium-bgp.md\":\"Dm6YW9na\",\"blogs_original_2024_01-crossplane.md\":\"CZV5SzPN\",\"blogs_original_2024_02-sidecar-containers.md\":\"BcdNsqvp\",\"blogs_original_2024_03-higress-ai-plugins.md\":\"vDs7YXRe\",\"blogs_original_2025_01-mcp.md\":\"7g8DiqYg\",\"blogs_original_2025_02-elasticsearch-mcp-server.md\":\"CbHAWkOF\",\"blogs_original_2025_03-mcp-client.md\":\"B6zT5NhJ\",\"blogs_original_2025_04-mcp-sse.md\":\"DGOHLBrN\",\"blogs_original_2025_05-higress-vs-oneapi.md\":\"BPz4hQhA\",\"blogs_original_2025_06-higress-ai-gateway.md\":\"CdD0d_sZ\",\"blogs_original_2025_07-deepseek-flashmla.md\":\"ByBUgK87\",\"blogs_original_2025_08-deepseek-deepep.md\":\"Buo5mS4U\",\"blogs_original_2025_09-deepseek-deepgemm.md\":\"DflR4oeE\",\"blogs_original_2025_10-deepseek-parallelism.md\":\"DsliyTZC\",\"blogs_original_2025_11-deepseek-3fs.md\":\"DLeFjMAh\",\"blogs_original_2025_12-manus-ai.md\":\"OaTFb2Vt\",\"blogs_original_2025_13-gpu-kind-cluster.md\":\"Xf6AHVqU\",\"blogs_original_2025_14-gateway-api-inference-extension.md\":\"Bptd5Akh\",\"blogs_original_2025_15-rag-higress-es-langchain.md\":\"C-xAnRRM\",\"blogs_original_2025_16-context7.md\":\"9HEyRqjV\",\"blogs_original_2025_17-runai-model-streamer.md\":\"B6JjOX4s\",\"blogs_original_2025_18-higress-llmaz-vllm.md\":\"CpmeUO3n\",\"blogs_original_2025_19-oceanbase-dify-mcp.md\":\"Ds23OjaC\",\"blogs_original_2025_20-dynamo.md\":\"eMPNl8_b\",\"blogs_original_index.md\":\"C5At_tbC\",\"blogs_translate_2023_01-life-of-a-packet-in-kubernetes-part-1.md\":\"UBr4Hvt3\",\"blogs_translate_2023_02-life-of-a-packet-in-kubernetes-part-2.md\":\"CLjiSiXz\",\"blogs_translate_2023_03-life-of-a-packet-in-kubernetes-part-3.md\":\"CSv_YjJ4\",\"blogs_translate_2023_04-life-of-a-packet-in-kubernetes-part-4.md\":\"DnsbTu7_\",\"blogs_translate_2023_05-git-cheat-sheet-1.md\":\"CL_FKJpj\",\"blogs_translate_2023_06-git-cheat-sheet-2.md\":\"BNt4Xq_P\",\"blogs_translate_2023_07-git-cheat-sheet-3.md\":\"BqCuRWlW\",\"blogs_translate_index.md\":\"DKB0bb0s\",\"categories_fragments_index.md\":\"66cEdavn\",\"categories_fragments_个人速查手册_01-git.md\":\"Fhbj9vJ9\",\"categories_fragments_个人速查手册_02-find-docker-file.md\":\"BzdliHLM\",\"categories_fragments_个人速查手册_03-idea-shortcut.md\":\"ftEVIvDk\",\"categories_fragments_个人速查手册_04-ssh-proxy.md\":\"BB6lyvvR\",\"categories_fragments_个人速查手册_05-docker-command.md\":\"Cpf_iunl\",\"categories_fragments_个人速查手册_06-kubernetes-command.md\":\"xl8PSR4K\",\"categories_fragments_个人速查手册_07.uv.md\":\"BtCmdkIP\",\"categories_fragments_个人速查手册_08-golang.md\":\"CVf1iqoH\",\"categories_fragments_个人速查手册_09-macos.md\":\"D4qxD-w1\",\"categories_fragments_个人速查手册_10-linux.md\":\"DQaynLL0\",\"categories_issues_bug万象集_01-ebpf.md\":\"3ky-sRpj\",\"categories_issues_bug万象集_02-ide.md\":\"SK7DvC1o\",\"categories_issues_index.md\":\"CzclY25Y\",\"categories_learning_ai_01-ai.md\":\"8xOin7ds\",\"categories_learning_ai_02-gpu.md\":\"CeZnQ6Pr\",\"categories_learning_ai_03-pytorch.md\":\"Ckrk6mM9\",\"categories_learning_ai_04-mcp.md\":\"Duok1stf\",\"categories_learning_ai_05-coze.md\":\"DKM5wo7t\",\"categories_learning_ebpf_01-ebpf.md\":\"5DfEnVgK\",\"categories_learning_ebpf_02-bpftool.md\":\"CMABDxeA\",\"categories_learning_index.md\":\"hYZprUiT\",\"categories_learning_云原生_01-istio.md\":\"DUgMPc_u\",\"categories_learning_云原生_02-observability.md\":\"DpjNkxqr\",\"categories_learning_测试_01-pytest.md\":\"Cj5hC2-K\",\"categories_learning_编程语言_01-rust.md\":\"BWFW8J-c\",\"categories_learning_编程语言_02-golang-concurrency.md\":\"MJlprN9e\",\"categories_learning_编程语言_02-golang-gmp.md\":\"DBqKZfF9\",\"categories_learning_编程语言_03-design-pattern.md\":\"D-Ql4NTs\",\"categories_open-source_index.md\":\"BW6HepNl\",\"categories_open-source_开源项目_01-ai.md\":\"CWACeSZ4\",\"categories_open-source_开源项目_02-web.md\":\"BHghET9Z\",\"categories_open-source_开源项目_03-mcp.md\":\"Bei97EhJ\",\"categories_open-source_开源项目_04-cloud-native.md\":\"C1-E1GwT\",\"categories_tools_index.md\":\"D6WShYDV\",\"categories_tools_云原生_01-k8s-resource.md\":\"VUN8-uh6\",\"courses_ai-infra_ai infra 教程_01-vllm-quickstart.md\":\"DgNd68wD\",\"courses_ai-infra_ai infra 教程_02-pagedattention.md\":\"aeCpdA2X\",\"courses_ai-infra_ai infra 教程_03-prefix-caching.md\":\"DHswtt1E\",\"courses_ai-infra_ai infra 教程_04-speculative-decoding.md\":\"Bi_3U6jp\",\"courses_ai-infra_ai infra 教程_05-chunked-prefills.md\":\"ltJxm0oo\",\"courses_ai-infra_ai infra 教程_06-disaggregating-prefill-and-decoding.md\":\"CJsgR_Ns\",\"courses_ai-infra_index.md\":\"Cu1hxydb\",\"courses_algorithm_index.md\":\"2pq-zHQ0\",\"courses_algorithm_算法_01-hot100.md\":\"B3iy4pGG\",\"courses_algorithm_算法_02-template.md\":\"NlPVfyuv\",\"courses_algorithm_算法_03-design-problem.md\":\"mFrKoGww\",\"courses_algorithm_算法_04-shell.md\":\"C56oKp8H\",\"courses_elastic-stack_elastic stack 实战教程_01-quick-start.md\":\"wjBUq_Lj\",\"courses_elastic-stack_elastic stack 实战教程_02-ilm.md\":\"CGPgWZ0I\",\"courses_elastic-stack_elastic stack 实战教程_03-snapshot.md\":\"BjH2UjWr\",\"courses_elastic-stack_elastic stack 实战教程_04-fleet.md\":\"BU-7UeMh\",\"courses_elastic-stack_elastic stack 实战教程_05-java-client.md\":\"cxgIfVgb\",\"courses_elastic-stack_index.md\":\"UB4mRszQ\",\"courses_interview_ai_01-ai.md\":\"B1mRKbWj\",\"courses_interview_elastic stack_01-elasticsearch.md\":\"D3nb-HFl\",\"courses_interview_index.md\":\"DJjyGpD6\",\"courses_interview_云原生_01-kubernetes.md\":\"Cg5TR9pG\",\"courses_interview_云原生_02-container.md\":\"BUgCeDw8\",\"courses_interview_云原生_03-storage.md\":\"BCP6RYl2\",\"courses_interview_云原生_04-network.md\":\"CzgvG0T8\",\"courses_interview_云原生_05-security.md\":\"C_JTBmpu\",\"courses_interview_云原生_06-rollout.md\":\"DyApC8la\",\"courses_interview_云原生_07-operator.md\":\"CE1YIAwU\",\"courses_interview_大数据_01-message-queue.md\":\"Cyc-hoEy\",\"courses_interview_操作系统_01-cpu.md\":\"BI9BuRpz\",\"courses_interview_操作系统_02-memory.md\":\"B9Zv2FjD\",\"courses_interview_操作系统_03-storage.md\":\"NZCAQPjb\",\"courses_interview_操作系统_04-network.md\":\"DKg_O8z0\",\"courses_interview_操作系统_05-cgroup.md\":\"D5MTgZMn\",\"courses_interview_编程语言_01-golang.md\":\"amekhOb5\",\"courses_interview_编程语言_02-python.md\":\"CjScczhZ\",\"courses_interview_编程语言_03-algorithm.md\":\"9ON_ExGQ\",\"courses_observability_opentelemetry_01-opentelemetry-elastic.md\":\"McqFmgCi\",\"courses_observability_index.md\":\"_pVJ70ky\",\"en_about_index.md\":\"BDN9Ytpe\",\"en_about_me.md\":\"tmC7v-Lx\",\"en_archives.md\":\"B8fP4SG-\",\"en_courses_elastic-stack_01-elastic-stack-hands-on-lab_01-quick-start.md\":\"DU8u-YJC\",\"en_courses_elastic-stack_01-elastic-stack-hands-on-lab_02-ilm.md\":\"BgZ4mtI2\",\"en_courses_elastic-stack_01-elastic-stack-hands-on-lab_03-snapshot.md\":\"CWveMj_h\",\"en_courses_elastic-stack_01-elastic-stack-hands-on-lab_04-fleet.md\":\"DJtUQ6Zm\",\"en_courses_elastic-stack_01-elastic-stack-hands-on-lab_05-java-client.md\":\"DaVe09ax\",\"en_courses_elastic-stack_index.md\":\"3vt3pdiK\",\"en_index.md\":\"DDs30Ujc\",\"en_tags.md\":\"cYJfUbxi\",\"index.md\":\"DNwaWdmo\",\"meetup_network_01-network_01-cni.md\":\"D3FS021B\",\"meetup_network_index.md\":\"CXPOTFDV\",\"tags.md\":\"4fYGVtWU\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"Se7en的架构笔记\",\"description\":\"个人技术知识库，记录 & 分享个人碎片化、结构化、体系化的技术知识内容。\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"博客\",\"items\":[{\"text\":\"原创\",\"link\":\"/blogs/original/index\",\"activeMatch\":\"/blogs/original/\"},{\"text\":\"翻译\",\"link\":\"/blogs/translate/index\",\"activeMatch\":\"/blogs/translate/\"}],\"activeMatch\":\"/blogs/\"},{\"text\":\"我的分类\",\"items\":[{\"text\":\"Bug万象集\",\"link\":\"/categories/issues/index\",\"activeMatch\":\"/categories/issues/\"},{\"text\":\"个人速查手册\",\"link\":\"/categories/fragments/index\",\"activeMatch\":\"/categories/fragments/\"},{\"text\":\"精选工具箱\",\"link\":\"/categories/tools/index\",\"activeMatch\":\"/categories/tools/\"},{\"text\":\"开源项目\",\"link\":\"/categories/open-source/index\",\"activeMatch\":\"/categories/open-source/\"},{\"text\":\"学习笔记\",\"link\":\"/categories/learning/index\",\"activeMatch\":\"/categories/learning/\"}],\"activeMatch\":\"/categories/\"},{\"text\":\"我的小册\",\"items\":[{\"text\":\"AI Infra 教程\",\"link\":\"/courses/ai-infra/index\",\"activeMatch\":\"/courses/ai-infra/\"},{\"text\":\"Elastic Stack 实战教程\",\"link\":\"/courses/elastic-stack/index\",\"activeMatch\":\"/courses/elastic-stack/\"},{\"text\":\"Observability 教程\",\"link\":\"/courses/observability/index\",\"activeMatch\":\"/courses/observability/\"},{\"text\":\"面试宝典\",\"link\":\"/courses/interview/index\",\"activeMatch\":\"/courses/interview/\"},{\"text\":\"数据结构与算法\",\"link\":\"/courses/algorithm/index\",\"activeMatch\":\"/courses/algorithm/\"}],\"activeMatch\":\"/courses/\"},{\"text\":\"我的标签\",\"link\":\"/tags\",\"activeMatch\":\"/tags\"},{\"text\":\"我的归档\",\"link\":\"/archives\",\"activeMatch\":\"/archives\"},{\"text\":\"关于\",\"items\":[{\"text\":\"关于知识库\",\"link\":\"/about/index\",\"activeMatch\":\"/about/index\"},{\"text\":\"关于我\",\"link\":\"/about/me\",\"activeMatch\":\"/about/me\"}],\"activeMatch\":\"/about/\"}],\"sidebar\":{\"/categories/issues/\":[{\"text\":\"Bug万象集 (2篇)\",\"items\":[{\"text\":\"1. eBPF\",\"link\":\"/categories/issues/Bug万象集/01-ebpf\"},{\"text\":\"2. IDE\",\"link\":\"/categories/issues/Bug万象集/02-ide\"}],\"collapsed\":false}],\"/categories/fragments/\":[{\"text\":\"个人速查手册 (10篇)\",\"items\":[{\"text\":\"1. Git 速查手册\",\"link\":\"/categories/fragments/个人速查手册/01-git\"},{\"text\":\"2. 5 种快速查找容器中文件的方法\",\"link\":\"/categories/fragments/个人速查手册/02-find-docker-file\"},{\"text\":\"3. IDEA 快捷键\",\"link\":\"/categories/fragments/个人速查手册/03-idea-shortcut\"},{\"text\":\"4. 使用 ClashX 代理 SSH 连接\",\"link\":\"/categories/fragments/个人速查手册/04-ssh-proxy\"},{\"text\":\"5. Docker 常用命令\",\"link\":\"/categories/fragments/个人速查手册/05-docker-command\"},{\"text\":\"6. Kubernetes 常用命令\",\"link\":\"/categories/fragments/个人速查手册/06-kubernetes-command\"},{\"text\":\"7. uv 速查手册\",\"link\":\"/categories/fragments/个人速查手册/07.uv\"},{\"text\":\"8. Golang 速查手册\",\"link\":\"/categories/fragments/个人速查手册/08-golang\"},{\"text\":\"9. MacOS 速查手册\",\"link\":\"/categories/fragments/个人速查手册/09-macos\"},{\"text\":\"10. Linux 速查手册\",\"link\":\"/categories/fragments/个人速查手册/10-linux\"}],\"collapsed\":false}],\"/categories/tools/\":[{\"text\":\"云原生 (1篇)\",\"items\":[{\"text\":\"1. 查询 Kubernetes 核心资源字段\",\"link\":\"/categories/tools/云原生/01-k8s-resource\"}],\"collapsed\":false}],\"/categories/learning/\":[{\"text\":\"AI (5篇)\",\"items\":[{\"text\":\"1. AI\",\"link\":\"/categories/learning/AI/01-ai\"},{\"text\":\"2. GPU\",\"link\":\"/categories/learning/AI/02-gpu\"},{\"text\":\"3. PyTorch\",\"link\":\"/categories/learning/AI/03-pytorch\"},{\"text\":\"4. Model Context Protocol（MCP）\",\"link\":\"/categories/learning/AI/04-mcp\"},{\"text\":\"5. Coze AI 开发平台\",\"link\":\"/categories/learning/AI/05-coze\"}],\"collapsed\":false},{\"text\":\"eBPF (2篇)\",\"items\":[{\"text\":\"1. eBPF 基础\",\"link\":\"/categories/learning/eBPF/01-ebpf\"},{\"text\":\"2. bpftool\",\"link\":\"/categories/learning/eBPF/02-bpftool\"}],\"collapsed\":false},{\"text\":\"云原生 (2篇)\",\"items\":[{\"text\":\"1. Istio\",\"link\":\"/categories/learning/云原生/01-istio\"},{\"text\":\"2. Observability\",\"link\":\"/categories/learning/云原生/02-observability\"}],\"collapsed\":false},{\"text\":\"测试 (1篇)\",\"items\":[{\"text\":\"1. Pytest\",\"link\":\"/categories/learning/测试/01-pytest\"}],\"collapsed\":false},{\"text\":\"编程语言 (4篇)\",\"items\":[{\"text\":\"1. Rust\",\"link\":\"/categories/learning/编程语言/01-rust\"},{\"text\":\"2. Go 并发编程\",\"link\":\"/categories/learning/编程语言/02-golang-concurrency\"},{\"text\":\"3. Go GMP\",\"link\":\"/categories/learning/编程语言/02-golang-gmp\"},{\"text\":\"4. 设计模式\",\"link\":\"/categories/learning/编程语言/03-design-pattern\"}],\"collapsed\":false}],\"/categories/open-source/\":[{\"text\":\"开源项目 (4篇)\",\"items\":[{\"text\":\"1. AI\",\"link\":\"/categories/open-source/开源项目/01-ai\"},{\"text\":\"2. Web\",\"link\":\"/categories/open-source/开源项目/02-web\"},{\"text\":\"3. MCP\",\"link\":\"/categories/open-source/开源项目/03-mcp\"},{\"text\":\"4. 云原生\",\"link\":\"/categories/open-source/开源项目/04-cloud-native\"}],\"collapsed\":false}],\"/blogs/original/\":[{\"text\":\"<img class=\\\"chinese-zodiac\\\" style=\\\"position: static; vertical-align: middle; padding-bottom: 3px;\\\" src=\\\"/img/svg/chinese-zodiac/snake.svg\\\" title=\\\"蛇年\\\" alt=\\\"生肖\\\">\\n            2025年 (20篇)\",\"items\":[{\"text\":\"1. 使用 NVIDIA Dynamo 部署 PD 分离推理服务\",\"link\":\"/blogs/original//2025/20-dynamo\"},{\"text\":\"2. Dify + OceanBase + MCP：三剑合璧，轻松构建 RAG 应用\",\"link\":\"/blogs/original//2025/19-oceanbase-dify-mcp\"},{\"text\":\"3. 使用 Higress AI 网关代理 vLLM 推理服务\",\"link\":\"/blogs/original//2025/18-higress-llmaz-vllm\"},{\"text\":\"4. 使用 Run:ai Model Streamer 实现模型的高效加载\",\"link\":\"/blogs/original//2025/17-runai-model-streamer\"},{\"text\":\"5. AI 乱写代码怎么破？使用 Context7 MCP Server 让 AI 写出靠谱代码!\",\"link\":\"/blogs/original//2025/16-context7\"},{\"text\":\"6. 使用 LangChain + Higress + Elasticsearch 构建 RAG 应用\",\"link\":\"/blogs/original//2025/15-rag-higress-es-langchain\"},{\"text\":\"7. 为 Kubernetes 提供智能的 LLM 推理路由：Gateway API Inference Extension 深度解析\",\"link\":\"/blogs/original//2025/14-gateway-api-inference-extension\"},{\"text\":\"8. 一键部署 GPU Kind 集群，体验 vLLM 极速推理\",\"link\":\"/blogs/original//2025/13-gpu-kind-cluster\"},{\"text\":\"9. Manus 刷屏，全网求邀请码！这款 AI Agent 究竟有多强？\",\"link\":\"/blogs/original//2025/12-manus-ai\"},{\"text\":\"10. DeepSeek 开源周第五弹：3FS —— 专为 AI 训练和推理设计的分布式存储\",\"link\":\"/blogs/original//2025/11-deepseek-3fs\"},{\"text\":\"11. DeepSeek 开源周第四弹：DualPipe 和 EPLB —— 优化并行策略\",\"link\":\"/blogs/original//2025/10-deepseek-parallelism\"},{\"text\":\"12. DeepSeek 开源周第三弹：DeepGEMM —— 高效的 FP8 GEMM 库，核心代码仅 300 行！\",\"link\":\"/blogs/original//2025/09-deepseek-deepgemm\"},{\"text\":\"13. DeepSeek 开源周第二弹：DeepEP —— 首个 MoE 模型训练和推理的 EP 通信库\",\"link\":\"/blogs/original//2025/08-deepseek-deepep\"},{\"text\":\"14. DeepSeek 开源周第一弹：FlashMLA —— 大模型推理的“涡轮增压器”\",\"link\":\"/blogs/original//2025/07-deepseek-flashmla\"},{\"text\":\"15. 提升 AI 服务的稳定性：Higress AI 网关的降级功能介绍\",\"link\":\"/blogs/original//2025/06-higress-ai-gateway\"},{\"text\":\"16. AI 网关对决：Higress 与 OneAPI 的功能对比\",\"link\":\"/blogs/original//2025/05-higress-vs-oneapi\"},{\"text\":\"17. 构建基于 SSE 协议通信的 MCP Server 和 Client\",\"link\":\"/blogs/original//2025/04-mcp-sse\"},{\"text\":\"18. 快速上手：实现你的第一个 MCP Client\",\"link\":\"/blogs/original//2025/03-mcp-client\"},{\"text\":\"19. MCP Server 开发实战：无缝对接 LLM 和 Elasticsearch\",\"link\":\"/blogs/original//2025/02-elasticsearch-mcp-server\"},{\"text\":\"20. 一文带你入门 MCP（模型上下文协议）\",\"link\":\"/blogs/original//2025/01-mcp\"}],\"collapsed\":false},{\"text\":\"<img class=\\\"chinese-zodiac\\\" style=\\\"position: static; vertical-align: middle; padding-bottom: 3px;\\\" src=\\\"/img/svg/chinese-zodiac/dragon.svg\\\" title=\\\"龙年\\\" alt=\\\"生肖\\\">\\n            2024年 (3篇)\",\"items\":[{\"text\":\"1. 使用 Higress AI 插件对接通义千问大语言模型\",\"link\":\"/blogs/original//2024/03-higress-ai-plugins\"},{\"text\":\"2. 深入剖析 Kubernetes 原生 Sidecar 容器\",\"link\":\"/blogs/original//2024/02-sidecar-containers\"},{\"text\":\"3. Crossplane 实战：构建统一的云原生控制平面\",\"link\":\"/blogs/original//2024/01-crossplane\"}],\"collapsed\":false},{\"text\":\"<img class=\\\"chinese-zodiac\\\" style=\\\"position: static; vertical-align: middle; padding-bottom: 3px;\\\" src=\\\"/img/svg/chinese-zodiac/rabbit.svg\\\" title=\\\"兔年\\\" alt=\\\"生肖\\\">\\n            2023年 (10篇)\",\"items\":[{\"text\":\"1. 使用 Containerlab + Kind 快速部署 Cilium BGP 环境\",\"link\":\"/blogs/original//2023/10-cilium-bgp\"},{\"text\":\"2. 使用 ClusterResourceSet 为 Cluster API 集群自动安装 CNI 插件\",\"link\":\"/blogs/original//2023/09-clusterresourceset\"},{\"text\":\"3. Cilium 多集群 Cluster Mesh 介绍\",\"link\":\"/blogs/original//2023/08-cilium-cluster-mesh\"},{\"text\":\"4. vCluster -- 基于虚拟集群的多租户方案\",\"link\":\"/blogs/original//2023/07-vcluster\"},{\"text\":\"5. Kubernetes 多集群网络方案系列 2 -- Submariner 监控\",\"link\":\"/blogs/original//2023/06-kubernetes-multi-cluster-2\"},{\"text\":\"6. Kubernetes 多集群网络方案系列 1 -- Submariner 介绍\",\"link\":\"/blogs/original//2023/05-kubernetes-multi-cluster-1\"},{\"text\":\"7. 在 Kubernetes 中使用 Keycloak OIDC Provider 对用户进行身份验证\",\"link\":\"/blogs/original//2023/04-kubernetes-keycloak-oidc\"},{\"text\":\"8. 使用 Kubectl Patch 命令更新资源\",\"link\":\"/blogs/original//2023/03-kubectl-patch\"},{\"text\":\"9. WebAssembly 在云原生中的实践指南\",\"link\":\"/blogs/original//2023/02-wasm-in-cloud-native\"},{\"text\":\"10. ArgoCD 简明教程\",\"link\":\"/blogs/original//2023/01-argocd-quickstart\"}],\"collapsed\":false},{\"text\":\"<img class=\\\"chinese-zodiac\\\" style=\\\"position: static; vertical-align: middle; padding-bottom: 3px;\\\" src=\\\"/img/svg/chinese-zodiac/tiger.svg\\\" title=\\\"虎年\\\" alt=\\\"生肖\\\">\\n            2022年 (13篇)\",\"items\":[{\"text\":\"1. 使用 Envoy 作为前端代理\",\"link\":\"/blogs/original//2022/13-envoy-quickstart\"},{\"text\":\"2. 实现 LRU 缓存算法\",\"link\":\"/blogs/original//2022/12.lru\"},{\"text\":\"3. Nebula 分布式图数据库介绍\",\"link\":\"/blogs/original//2022/11.nebula\"},{\"text\":\"4. Habor 部署指南\",\"link\":\"/blogs/original//2022/10-harbor\"},{\"text\":\"5. Pulsar 介绍与部署\",\"link\":\"/blogs/original//2022/09-pulsar\"},{\"text\":\"6. 如何重置 Kafka 中的 Consumer Offset？\",\"link\":\"/blogs/original//2022/08-reset-kafka-consumer-offset\"},{\"text\":\"7. 如何往 Kafka 发送大消息？\",\"link\":\"/blogs/original//2022/07-kafka-big-message\"},{\"text\":\"8. Kafka 生产环境部署指南\",\"link\":\"/blogs/original//2022/06-kafka-setup\"},{\"text\":\"9. Docker Rootless 在非特权模式下运行 Docker\",\"link\":\"/blogs/original//2022/05-docker-rootless\"},{\"text\":\"10. 使用 ezctl 工具部署和管理 Kubernetes 集群\",\"link\":\"/blogs/original//2022/04-kubeasz\"},{\"text\":\"11. Kubernetes 中的对象是如何删除的：Finalizers 字段介绍\",\"link\":\"/blogs/original//2022/03-finalizer\"},{\"text\":\"12. Kubectl debug 调试容器\",\"link\":\"/blogs/original//2022/02-kubectl-debug\"},{\"text\":\"13. Kubenetes 高可用集群搭建\",\"link\":\"/blogs/original//2022/01-kubernetes-setup\"}],\"collapsed\":false}],\"/blogs/translate/\":[{\"text\":\"<img class=\\\"chinese-zodiac\\\" style=\\\"position: static; vertical-align: middle; padding-bottom: 3px;\\\" src=\\\"/img/svg/chinese-zodiac/rabbit.svg\\\" title=\\\"兔年\\\" alt=\\\"生肖\\\">\\n            2023年 (7篇)\",\"items\":[{\"text\":\"1. Git 速查表：专家必备的 14 个 Git 命令\",\"link\":\"/blogs/translate//2023/07-git-cheat-sheet-3\"},{\"text\":\"2. Git 速查表：中级用户必备的 12 个 Git 命令\",\"link\":\"/blogs/translate//2023/06-git-cheat-sheet-2\"},{\"text\":\"3. Git 速查表：初学者必备的 12 个 Git 命令\",\"link\":\"/blogs/translate//2023/05-git-cheat-sheet-1\"},{\"text\":\"4. Kubernetes 中数据包的生命周期 -- 第 4 部分\",\"link\":\"/blogs/translate//2023/04-life-of-a-packet-in-kubernetes-part-4\"},{\"text\":\"5. Kubernetes 中数据包的生命周期 -- 第 3 部分\",\"link\":\"/blogs/translate//2023/03-life-of-a-packet-in-kubernetes-part-3\"},{\"text\":\"6. Kubernetes 中数据包的生命周期 -- 第 2 部分\",\"link\":\"/blogs/translate//2023/02-life-of-a-packet-in-kubernetes-part-2\"},{\"text\":\"7. Kubernetes 中数据包的生命周期 -- 第 1 部分\",\"link\":\"/blogs/translate//2023/01-life-of-a-packet-in-kubernetes-part-1\"}],\"collapsed\":false}],\"/courses/elastic-stack/\":[{\"text\":\"Elastic Stack 实战教程 (5篇)\",\"items\":[{\"text\":\"1. Elastic Stack 8 快速上手\",\"link\":\"/courses/elastic-stack/Elastic Stack 实战教程/01-quick-start\"},{\"text\":\"2. ILM 索引生命周期管理\",\"link\":\"/courses/elastic-stack/Elastic Stack 实战教程/02-ilm\"},{\"text\":\"3. 快照备份与恢复\",\"link\":\"/courses/elastic-stack/Elastic Stack 实战教程/03-snapshot\"},{\"text\":\"4. 使用 Fleet 管理 Elastic Agent 监控应用\",\"link\":\"/courses/elastic-stack/Elastic Stack 实战教程/04-fleet\"},{\"text\":\"5. Elasticsearch Java API Client 开发\",\"link\":\"/courses/elastic-stack/Elastic Stack 实战教程/05-java-client\"}],\"collapsed\":false}],\"/courses/ai-infra/\":[{\"text\":\"AI Infra 教程 (6篇)\",\"items\":[{\"text\":\"1. vLLM 快速部署指南\",\"link\":\"/courses/ai-infra/AI Infra 教程/01-vllm-quickstart\"},{\"text\":\"2. vLLM 核心技术 PagedAttention 原理详解\",\"link\":\"/courses/ai-infra/AI Infra 教程/02-pagedattention\"},{\"text\":\"3. Prefix Caching 详解：实现 KV Cache 的跨请求高效复用\",\"link\":\"/courses/ai-infra/AI Infra 教程/03-prefix-caching\"},{\"text\":\"4. Speculative Decoding 推测解码方案详解\",\"link\":\"/courses/ai-infra/AI Infra 教程/04-speculative-decoding\"},{\"text\":\"5. Chunked Prefills 分块预填充详解\",\"link\":\"/courses/ai-infra/AI Infra 教程/05-chunked-prefills\"},{\"text\":\"6. PD 分离推理架构详解\",\"link\":\"/courses/ai-infra/AI Infra 教程/06-disaggregating-prefill-and-decoding\"}],\"collapsed\":false}],\"/courses/observability/\":[{\"text\":\"OpenTelemetry (1篇)\",\"items\":[{\"text\":\"1. OpenTelemetry × Elastic Observability 系列（一）：整体架构介绍\",\"link\":\"/courses/observability/OpenTelemetry/01-opentelemetry-elastic\"}],\"collapsed\":false}],\"/courses/interview/\":[{\"text\":\"AI (1篇)\",\"items\":[{\"text\":\"1. AI\",\"link\":\"/courses/interview/AI/01-ai\"}],\"collapsed\":false},{\"text\":\"Elastic Stack (1篇)\",\"items\":[{\"text\":\"1. Elasticsearch\",\"link\":\"/courses/interview/Elastic Stack/01-elasticsearch\"}],\"collapsed\":false},{\"text\":\"云原生 (7篇)\",\"items\":[{\"text\":\"1. Kubernetes\",\"link\":\"/courses/interview/云原生/01-kubernetes\"},{\"text\":\"2. 容器\",\"link\":\"/courses/interview/云原生/02-container\"},{\"text\":\"3. 存储\",\"link\":\"/courses/interview/云原生/03-storage\"},{\"text\":\"4. 网络\",\"link\":\"/courses/interview/云原生/04-network\"},{\"text\":\"5. 安全\",\"link\":\"/courses/interview/云原生/05-security\"},{\"text\":\"6. 发布\",\"link\":\"/courses/interview/云原生/06-rollout\"},{\"text\":\"7. Operator\",\"link\":\"/courses/interview/云原生/07-operator\"}],\"collapsed\":false},{\"text\":\"大数据 (1篇)\",\"items\":[{\"text\":\"1. 消息队列\",\"link\":\"/courses/interview/大数据/01-message-queue\"}],\"collapsed\":false},{\"text\":\"操作系统 (5篇)\",\"items\":[{\"text\":\"1. CPU\",\"link\":\"/courses/interview/操作系统/01-cpu\"},{\"text\":\"2. 内存\",\"link\":\"/courses/interview/操作系统/02-memory\"},{\"text\":\"3. 存储\",\"link\":\"/courses/interview/操作系统/03-storage\"},{\"text\":\"4. 网络\",\"link\":\"/courses/interview/操作系统/04-network\"},{\"text\":\"5. cgroup\",\"link\":\"/courses/interview/操作系统/05-cgroup\"}],\"collapsed\":false},{\"text\":\"编程语言 (3篇)\",\"items\":[{\"text\":\"1. Golang\",\"link\":\"/courses/interview/编程语言/01-golang\"},{\"text\":\"2. Python\",\"link\":\"/courses/interview/编程语言/02-python\"},{\"text\":\"3. 数据结构与算法\",\"link\":\"/courses/interview/编程语言/03-algorithm\"}],\"collapsed\":false}],\"/courses/algorithm/\":[{\"text\":\"算法 (4篇)\",\"items\":[{\"text\":\"1. Hot 100 算法题\",\"link\":\"/courses/algorithm/算法/01-hot100\"},{\"text\":\"2. 算法解题套路框架\",\"link\":\"/courses/algorithm/算法/02-template\"},{\"text\":\"3. 经典设计问题\",\"link\":\"/courses/algorithm/算法/03-design-problem\"},{\"text\":\"4. Shell 命令\",\"link\":\"/courses/algorithm/算法/04-shell\"}],\"collapsed\":false}],\"/meetup/network/\":[{\"text\":\"network (1篇)\",\"items\":[{\"text\":\"1. CNI (Container Network Interface)\",\"link\":\"/meetup/network/01-network/01-cni\"}],\"collapsed\":false}]},\"logo\":\"/logo.png\",\"outline\":{\"level\":\"deep\",\"label\":\"目录\"},\"darkModeSwitchLabel\":\"切换日光/暗黑模式\",\"sidebarMenuLabel\":\"文章\",\"returnToTopLabel\":\"返回顶部\",\"lastUpdatedText\":\"最后更新\",\"docFooter\":{\"prev\":\"上一篇\",\"next\":\"下一篇\"},\"editLink\":{\"pattern\":\"https://github.com/cr7258/cr7258.github.io/edit/main/docs/:path\",\"text\":\"不妥之处，敬请雅正\"},\"search\":{\"provider\":\"local\",\"options\":{\"locales\":{\"root\":{\"translations\":{\"button\":{\"buttonText\":\"搜索文档\",\"buttonAriaLabel\":\"搜索文档\"},\"modal\":{\"noResultsText\":\"无法找到相关结果\",\"resetButtonTitle\":\"清除查询条件\",\"footer\":{\"selectText\":\"选择\",\"navigateText\":\"切换\"}}}}}}},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/cr7258/cr7258.github.io\"}],\"articleMetadataConfig\":{\"author\":\"Se7en\",\"authorLink\":\"/about/me\",\"showViewCount\":false},\"copyrightConfig\":{\"license\":\"署名-相同方式共享 4.0 国际 (CC BY-SA 4.0)\",\"licenseLink\":\"http://creativecommons.org/licenses/by-sa/4.0/\"},\"commentConfig\":{\"type\":\"gitalk\",\"showComment\":true},\"footerConfig\":{\"showFooter\":true,\"copyright\":\"Copyright  2023-2025 Se7en\"}},\"locales\":{\"root\":{\"label\":\"中文\",\"lang\":\"zh\"},\"en\":{\"label\":\"English\",\"lang\":\"en\"}},\"scrollOffset\":90,\"cleanUrls\":true}");</script>
    
  </body>
</html>